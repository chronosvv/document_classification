{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备20类新闻数据样本，解压缩到news20文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, isdir, join\n",
    "import tarfile\n",
    "import gzip\n",
    "\n",
    "news20 = \"20news-bydate.tar.gz\"\n",
    "news20_path = 'news20'\n",
    "\n",
    "models_path = 'models'\n",
    "\n",
    "temp_path = 'temp'\n",
    "\n",
    "if not isdir(news20_path):\n",
    "    gz = gzip.GzipFile(news20)\n",
    "    tarfilename = news20.replace('.gz', '')\n",
    "    open(tarfilename, 'wb').write(gz.read())\n",
    "    \n",
    "    with tarfile.TarFile(tarfilename) as tarf:\n",
    "        tarf.extractall(news20_path)\n",
    "        tarf.close()\n",
    "    \n",
    "    gz.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到训练数据和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert isdir(news20_path), \"The news20 is not availaible\"\n",
    "\n",
    "train_path = news20_path + '/20news-bydate-train'\n",
    "test_path = news20_path + '/20news-bydate-test'\n",
    "\n",
    "def get_x_y():\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    \n",
    "    for foldername in os.listdir(train_path):\n",
    "        folder_path = join(train_path, foldername)\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            f = open(folder_path + '/' + filename, 'rb').read()\n",
    "            train_x.append(f)\n",
    "            train_y.append(foldername)\n",
    "            \n",
    "    for foldername in os.listdir(test_path):   \n",
    "        folder_path = join(test_path, foldername)\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            f = open(folder_path + '/' + filename, 'rb').read()\n",
    "            test_x.append(f)\n",
    "            test_y.append(foldername)\n",
    "            \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b\"From: mattf@cac.washington.edu (Matthew Freedman)\\nSubject: Non-Roman Font Availability\\nArticle-I.D.: shelley.1rmgleINNa0g\\nDistribution: world\\nOrganization: U.W. Information Systems\\nLines: 16\\nNNTP-Posting-Host: elvis.cac.washington.edu\\n\\nCan anybody tell me anything about the availibility of non-Roman fonts\\nfor X-Windows? Especially Unicode and/or han idiographic fonts.\\n\\nAlso, how about conversion tools for getting PC/Macintosh fonts into a\\nformat suitable for X? I would assume it is not too difficult for\\nbitmap fonts.\\n\\nThe FAQ's for this group and comp.fonts are not very helpful on these\\nquestions. \\n\\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\\n= Matthew M. Freedman                                                 =\\n= U. of Washington Information Systems       mattf@cac.washington.edu =\\n= 4545 15th Ave. NE; 3rd Floor               (206) 543-5593           =\\n= Seattle, WA  98105                                                  =\\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\\n\"]\n",
      "['comp.windows.x']\n",
      "训练集一共有文档11314\n",
      "测试集一共有文档7532\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = get_x_y()\n",
    "print(test_x[:1])\n",
    "print(train_y[:1])\n",
    "print(\"训练集一共有文档{}\".format(len(train_x)))\n",
    "print(\"测试集一共有文档{}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去掉数字，非字符以及停用词，并且全部转换为小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def new_corpus(corpus):\n",
    "    stoplist = 'for of the and to in'.split()\n",
    "    stoplist.append('')\n",
    "    new_corpus = [word for word in re.split(r'\\W+', corpus.lower()) if word not in stoplist and not re.match(r'[0-9]+', word)]\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'afielden', 'cbnewsb', 'cb', 'att', 'com', 'andrew', 'j', 'fielden', 'subject', 'x', 'benchmarks', 'keywords', 'benchmark', 'organization', 'at', 't', 'lines', 'we', 'are', 'process', 'evaluating', 'x', 'terminals', 'this', 'includes', 'running', 'xremote', 'over', 'a', 'serial', 'line', 'i', 'would', 'like', 'run', 'some', 'x', 'benchmarks', 'determine', 'comparative', 'performance', 'has', 'anyone', 'written', 'any', 'such', 'benchmarks', 'or', 'know', 'any', 'useful', 'programs', 'on', 'net', 'i', 'heard', 'a', 'program', 'called', 'xstone', 'but', 'i', 'couldn', 't', 'locate', 'it', 'using', 'archie', 'please', 'reply', 'afielden', 'mlsma', 'att', 'com', 'as', 'i', 'don', 't', 'get', 'read', 'this', 'newsgroup', 'much', 'thanks', 'advance', 'any', 'help', 'andrew', 'fielden', 'at', 't', 'network', 'systems', 'uk', 'tel', 'information', 'systems', 'group', 'sun', 'support', 'email', 'afielden', 'mlsma', 'att', 'com']\n",
      "['from', 'mattf', 'cac', 'washington', 'edu', 'matthew', 'freedman', 'subject', 'non', 'roman', 'font', 'availability', 'article', 'i', 'd', 'shelley', 'distribution', 'world', 'organization', 'u', 'w', 'information', 'systems', 'lines', 'nntp', 'posting', 'host', 'elvis', 'cac', 'washington', 'edu', 'can', 'anybody', 'tell', 'me', 'anything', 'about', 'availibility', 'non', 'roman', 'fonts', 'x', 'windows', 'especially', 'unicode', 'or', 'han', 'idiographic', 'fonts', 'also', 'how', 'about', 'conversion', 'tools', 'getting', 'pc', 'macintosh', 'fonts', 'into', 'a', 'format', 'suitable', 'x', 'i', 'would', 'assume', 'it', 'is', 'not', 'too', 'difficult', 'bitmap', 'fonts', 'faq', 's', 'this', 'group', 'comp', 'fonts', 'are', 'not', 'very', 'helpful', 'on', 'these', 'questions', 'matthew', 'm', 'freedman', 'u', 'washington', 'information', 'systems', 'mattf', 'cac', 'washington', 'edu', 'ave', 'ne', 'floor', 'seattle', 'wa']\n"
     ]
    }
   ],
   "source": [
    "def get_new_x_y():\n",
    "    train_x, train_y, test_x, test_y = get_x_y()\n",
    "    new_train_x = [new_corpus(x.decode('gbk', 'ignore')) for x in train_x]\n",
    "    new_test_x = [new_corpus(x.decode('gbk', 'ignore')) for x in test_x]\n",
    "    return new_train_x, new_test_x\n",
    "\n",
    "new_train_x, new_test_x = get_new_x_y()\n",
    "print(new_train_x[0])\n",
    "print(new_test_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]]\n",
      "[5 5 5]\n",
      "[5 5 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(train_y)\n",
    "\n",
    "train_y_lb = lb.transform(train_y)\n",
    "test_y_lb = lb.transform(test_y)\n",
    "\n",
    "le = preprocessing.LabelEncoder() \n",
    "le.fit(train_y)\n",
    "train_y_le = le.transform(train_y)\n",
    "test_y_le = le.transform(test_y)\n",
    "\n",
    "print(test_y_lb[-2:])\n",
    "print(test_y_le[:3])\n",
    "print(train_y_le[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集、测试集数据分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad7dc9b518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad7d72d278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # 使得我们可以对DataFrame使用display()函数\n",
    "from pandas import Series, DataFrame\n",
    "import math\n",
    "\n",
    "# 设置以内联的形式显示matplotlib绘制的图片（在notebook中显示更美观）\n",
    "%matplotlib inline\n",
    "def distribution():\n",
    "    train_x, train_y, test_x, test_y = get_x_y()\n",
    "    new_train_x, new_test_x = get_new_x_y()\n",
    "    \n",
    "    train_x_len = [len(doc) for doc in new_train_x]\n",
    "    test_x_len = [len(doc) for doc in new_test_x]\n",
    "    \n",
    "    train_y_type = {}\n",
    "    test_y_type = {}\n",
    "    for doc in train_y:\n",
    "        if doc not in train_y_type:\n",
    "            train_y_type[doc] = 1\n",
    "        else:\n",
    "            train_y_type[doc] += 1\n",
    "            \n",
    "    for doc in test_y:\n",
    "        if doc not in test_y_type:\n",
    "            test_y_type[doc] = 1\n",
    "        else:\n",
    "            test_y_type[doc] += 1\n",
    "    \n",
    "     #文档长度\n",
    "    fig, axe = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "    for i in range(2):\n",
    "        axe[i].set_title(\"document length distribution\")\n",
    "        axe[i].set_xlabel(\"document length(log10)\")\n",
    "        axe[i].set_ylabel(\"percent of document length(%)\")\n",
    "        \n",
    "    axe[0].hist(np.log10(Series(train_x_len)), normed = True, alpha = 0.75,color = 'g')    \n",
    "    axe[1].hist(np.log10(Series(test_x_len)), normed = True, alpha = 0.75,color = 'g')\n",
    "\n",
    "    fig1, axe1 = plt.subplots(1, 2, sharex=True, sharey=True)        \n",
    "    plt.subplot(121)\n",
    "    Series(train_y_type).plot(alpha = 0.75, kind = 'bar')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    Series(test_y_type).plot(alpha = 0.75, kind = 'bar')\n",
    "\n",
    "distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练集按2:8拆成验证集和训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "new_train_x, train_y, test_size=0.2)\n",
    "\n",
    "y_train_lb = lb.transform(y_train)\n",
    "y_val_lb = lb.transform(y_val)\n",
    "y_train_le = le.transform(y_train)\n",
    "y_val_le = le.transform(y_val)\n",
    "\n",
    "print(y_train_lb[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dictionary\n",
      "字典共有单词107010个\n"
     ]
    }
   ],
   "source": [
    "reflesh = False\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import corpora\n",
    "def make_dictionary():\n",
    "    new_train_x, new_test_x = get_new_x_y()\n",
    "    \n",
    "    #创建字典\n",
    "    assert isdir(temp_path), os.mkdir(temp_path)\n",
    "    if isfile(temp_path + '/20news.dict') and (reflesh == False):\n",
    "        print(\"loading dictionary\")\n",
    "        dictionary = corpora.Dictionary.load(temp_path + '/20news.dict')\n",
    "    else:\n",
    "        dictionary = corpora.Dictionary(new_train_x)\n",
    "        dictionary.save(temp_path + '/20news.dict')\n",
    "    return dictionary\n",
    "dictionary = make_dictionary()\n",
    "print(\"字典共有单词{}个\".format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成词袋子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_x_y():\n",
    "    bow_train_x = [ dictionary.doc2bow(doc) for doc in new_train_x]\n",
    "    bow_test_x = [dictionary.doc2bow(doc) for doc in new_test_x]\n",
    "    bow_x_train = [dictionary.doc2bow(doc) for doc in x_train]\n",
    "    bow_x_val = [dictionary.doc2bow(doc) for doc in x_val]\n",
    "    \n",
    "    return bow_train_x, bow_test_x, bow_x_train, bow_x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 6), (4, 1), (5, 2), (9, 3), (13, 1), (17, 4), (21, 1), (22, 1), (25, 1), (30, 3)]\n",
      "[(0, 1), (7, 1), (25, 1), (27, 1), (31, 2)]\n",
      "[(0, 30), (4, 3), (7, 11), (8, 9), (9, 22), (13, 18), (14, 5), (21, 2), (25, 15), (26, 5), (27, 3), (28, 5), (29, 1), (30, 1), (31, 68), (34, 18), (37, 4), (38, 1), (40, 1), (43, 2), (47, 31), (48, 5), (49, 1), (50, 2), (56, 4), (61, 4), (62, 1), (65, 1), (67, 17), (75, 41), (76, 8), (77, 1), (81, 3), (82, 3), (83, 6), (84, 2), (90, 16), (93, 5), (97, 3), (99, 5), (103, 4), (106, 1), (109, 4), (116, 2), (119, 6), (120, 1), (123, 6), (135, 15), (137, 18), (145, 1), (157, 1), (166, 10), (167, 13), (169, 4), (173, 9), (179, 6), (180, 2), (181, 4), (183, 13), (184, 2), (188, 2), (190, 5), (206, 2), (209, 2), (212, 1), (222, 1), (229, 3), (230, 3), (245, 1), (246, 5), (250, 3), (255, 1), (260, 1), (261, 1), (264, 1), (265, 41), (275, 3), (281, 42), (284, 2), (285, 14), (286, 6), (291, 1), (292, 6), (301, 1), (305, 5), (313, 4), (317, 1), (318, 50), (322, 1), (328, 4), (329, 1), (331, 6), (332, 2), (334, 2), (335, 4), (336, 2), (341, 1), (344, 8), (347, 2), (366, 9), (369, 3), (376, 2), (377, 1), (384, 1), (398, 6), (400, 3), (402, 6), (409, 8), (410, 1), (416, 4), (420, 3), (421, 2), (423, 13), (425, 3), (427, 4), (429, 4), (430, 7), (431, 2), (435, 3), (436, 4), (437, 18), (446, 1), (450, 1), (453, 1), (454, 1), (455, 2), (457, 12), (458, 1), (460, 4), (468, 1), (482, 4), (488, 3), (489, 11), (491, 1), (493, 1), (498, 2), (500, 1), (509, 4), (516, 2), (518, 1), (520, 2), (525, 1), (527, 1), (533, 1), (539, 2), (543, 1), (554, 1), (555, 1), (557, 1), (571, 28), (604, 5), (611, 2), (636, 5), (643, 5), (661, 3), (662, 1), (674, 2), (680, 3), (689, 4), (696, 2), (709, 1), (725, 1), (735, 2), (736, 1), (744, 10), (752, 1), (753, 2), (767, 1), (783, 1), (786, 2), (793, 1), (814, 4), (817, 1), (818, 2), (829, 1), (832, 1), (833, 8), (834, 4), (835, 23), (855, 1), (859, 3), (860, 3), (873, 1), (880, 2), (895, 7), (907, 4), (919, 1), (946, 1), (948, 6), (967, 2), (987, 1), (988, 1), (1020, 2), (1038, 1), (1040, 4), (1041, 1), (1046, 2), (1071, 1), (1082, 1), (1085, 15), (1086, 1), (1098, 1), (1099, 1), (1109, 1), (1113, 14), (1130, 8), (1154, 1), (1156, 2), (1164, 1), (1165, 5), (1179, 7), (1193, 2), (1237, 1), (1250, 1), (1266, 2), (1295, 2), (1297, 4), (1311, 1), (1313, 9), (1323, 2), (1340, 1), (1341, 2), (1358, 1), (1366, 2), (1369, 4), (1371, 1), (1372, 1), (1378, 1), (1410, 2), (1416, 1), (1473, 1), (1482, 2), (1488, 1), (1497, 3), (1504, 33), (1505, 7), (1510, 3), (1513, 1), (1527, 1), (1535, 1), (1543, 2), (1556, 1), (1641, 2), (1650, 1), (1663, 2), (1664, 1), (1678, 1), (1694, 2), (1720, 2), (1739, 9), (1767, 2), (1838, 3), (1901, 1), (1944, 1), (1970, 5), (2113, 1), (2124, 11), (2130, 1), (2131, 1), (2146, 1), (2147, 1), (2164, 1), (2171, 2), (2229, 7), (2267, 1), (2268, 3), (2269, 3), (2280, 2), (2285, 3), (2304, 7), (2305, 2), (2359, 1), (2386, 1), (2396, 1), (2437, 2), (2455, 2), (2461, 1), (2468, 1), (2474, 1), (2485, 2), (2511, 1), (2517, 9), (2577, 6), (2580, 1), (2607, 2), (2616, 24), (2682, 1), (2709, 7), (2712, 5), (2714, 1), (2754, 1), (2765, 1), (2825, 4), (2836, 1), (2862, 3), (2866, 2), (2935, 6), (2950, 1), (2959, 3), (2996, 2), (3002, 3), (3011, 1), (3021, 1), (3033, 2), (3086, 1), (3098, 1), (3101, 1), (3107, 1), (3127, 1), (3135, 6), (3143, 3), (3146, 1), (3190, 1), (3203, 3), (3219, 3), (3244, 4), (3307, 2), (3325, 1), (3409, 1), (3482, 1), (3516, 1), (3554, 1), (3616, 1), (3617, 2), (3668, 1), (3694, 2), (3700, 4), (3769, 5), (3790, 1), (3805, 1), (3934, 1), (4004, 1), (4007, 2), (4184, 1), (4202, 2), (4273, 1), (4297, 1), (4327, 1), (4349, 1), (4356, 1), (4428, 7), (4452, 1), (4580, 2), (4583, 1), (4609, 11), (4645, 1), (4736, 1), (4776, 1), (4832, 1), (4852, 1), (4878, 5), (5044, 1), (5069, 1), (5133, 3), (5146, 1), (5244, 4), (5253, 1), (5280, 1), (5308, 1), (5332, 2), (5431, 1), (5504, 2), (5512, 2), (5531, 1), (5677, 1), (5835, 1), (5842, 2), (5933, 1), (5976, 1), (5980, 7), (6040, 1), (6096, 1), (6142, 2), (6167, 1), (6221, 1), (6241, 9), (6309, 1), (6331, 1), (6449, 1), (6480, 2), (6484, 3), (6493, 1), (6524, 1), (6542, 1), (6661, 3), (6801, 1), (6865, 1), (6934, 1), (7021, 2), (7022, 1), (7049, 1), (7062, 1), (7123, 1), (7501, 6), (7526, 2), (7527, 2), (7584, 1), (7746, 4), (7797, 1), (7817, 3), (8023, 1), (8189, 2), (8192, 1), (8215, 1), (8417, 1), (8420, 1), (8559, 3), (8577, 1), (8634, 1), (8846, 1), (8902, 2), (8934, 1), (9120, 1), (9136, 1), (9299, 2), (9379, 1), (9566, 1), (9586, 5), (9617, 1), (9717, 1), (9739, 1), (9878, 1), (10086, 1), (10088, 1), (10168, 1), (10176, 1), (10188, 1), (10561, 1), (10646, 1), (10717, 1), (10750, 1), (10913, 1), (10974, 1), (11105, 2), (11255, 1), (11299, 2), (11380, 1), (11626, 1), (11646, 10), (11912, 1), (12107, 3), (12297, 1), (12608, 7), (12708, 1), (12782, 2), (12791, 1), (12810, 3), (12827, 1), (12858, 1), (12933, 1), (12999, 1), (13023, 1), (13067, 3), (13106, 1), (13112, 2), (13155, 1), (13234, 2), (13245, 1), (13266, 1), (13286, 1), (13426, 1), (13596, 1), (13603, 1), (13643, 3), (13711, 1), (13745, 3), (13757, 2), (13838, 1), (13847, 1), (13882, 1), (14076, 1), (14084, 1), (14096, 1), (14106, 1), (14118, 1), (14135, 2), (14232, 4), (14265, 2), (14279, 2), (14400, 1), (14458, 1), (14651, 13), (14714, 1), (14725, 2), (14823, 1), (14893, 1), (14933, 1), (14950, 2), (15032, 1), (15050, 1), (15079, 1), (15135, 1), (15287, 1), (15343, 1), (15457, 1), (15461, 8), (15565, 1), (15580, 2), (15620, 1), (15660, 1), (15832, 1), (15865, 3), (15876, 1), (15897, 1), (15949, 1), (15972, 1), (16050, 2), (16140, 1), (16275, 1), (16284, 1), (16305, 2), (16391, 1), (16405, 1), (16470, 3), (16908, 1), (16986, 1), (17257, 1), (17542, 1), (17666, 1), (17712, 4), (17887, 1), (18512, 1), (18731, 1), (18832, 2), (18845, 1), (18857, 2), (18888, 1), (18938, 1), (19232, 1), (19679, 4), (19795, 1), (20263, 1), (20272, 6), (20291, 1), (20310, 1), (20315, 1), (20495, 1), (20910, 2), (21299, 1), (21380, 1), (21453, 1), (21540, 1), (21559, 1), (21707, 1), (21769, 1), (21861, 5), (21981, 1), (22055, 1), (22114, 1), (22275, 1), (22562, 1), (22854, 1), (22947, 1), (23459, 1), (23936, 1), (24038, 1), (24179, 1), (24202, 1), (24206, 1), (24233, 1), (24365, 1), (24513, 1), (24595, 2), (24649, 1), (24736, 1), (24928, 1), (25162, 2), (25232, 1), (25429, 2), (25508, 1), (25595, 1), (25917, 1), (26183, 1), (26465, 2), (26894, 2), (27130, 1), (27626, 9), (27840, 1), (28064, 7), (28085, 1), (28683, 2), (29518, 1), (29591, 1), (29854, 1), (30796, 3), (30938, 1), (31904, 1), (32187, 1), (32245, 2), (33043, 5), (33044, 13), (33239, 2), (33274, 2), (33277, 12), (33300, 2), (33311, 2), (33384, 1), (33387, 1), (33392, 12), (33395, 4), (34343, 1), (34535, 1), (34600, 1), (34685, 4), (34690, 1), (34955, 2), (34964, 1), (34975, 3), (34980, 1), (34989, 2), (34990, 9), (35086, 1), (35142, 3), (35534, 1), (35745, 2), (36119, 2), (36138, 1), (36145, 1), (36490, 1), (36743, 1), (37766, 2), (37767, 1), (37807, 1), (37829, 1), (37895, 2), (37896, 2), (37944, 1), (38218, 1), (38277, 4), (38307, 1), (38311, 1), (38332, 1), (38353, 1), (38356, 1), (38561, 1), (38809, 1), (38810, 1), (38811, 1), (38812, 1), (38813, 1), (38814, 1), (38815, 1), (38816, 1), (38817, 1), (38818, 1), (38819, 1), (38820, 1), (38821, 1), (38822, 1), (38823, 2), (38824, 1), (38825, 1), (38826, 1), (38827, 1), (38828, 1), (38829, 1), (38830, 1), (38831, 1), (38832, 1), (38833, 1), (38834, 1), (38835, 1), (38836, 1), (38837, 1), (38838, 1), (38839, 1), (38840, 1), (38841, 1), (38842, 1), (38843, 1), (38844, 1)]\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "bow_train_x, bow_test_x, bow_x_train, bow_x_val= get_bow_x_y()\n",
    "print(bow_train_x[1][:10])\n",
    "print(bow_test_x[0][:5])\n",
    "print(bow_x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成tfidf模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.014779633588242457), (4, 0.0287460826966642), (7, 0.018347744445060897), (13, 0.018838397671037847), (31, 0.01454616459833816), (40, 8.243039588854396e-05), (47, 0.013988245131503171), (49, 0.001146508846542632), (67, 0.035307068432019174), (71, 0.07259638296125336), (72, 0.08403335489434896), (93, 0.022662867858653887), (103, 0.08467269000068807), (123, 0.01585055656129969), (148, 0.010725264179276354), (157, 0.02891058746219916), (181, 0.09435365224821175), (188, 0.0314095215244517), (204, 0.02438212330097529), (208, 0.024755228234037818), (213, 0.023056550946465103), (283, 0.026497064114131427), (305, 0.034863269983278676), (402, 0.03787263699535644), (420, 0.06541724236350498), (423, 0.030859616718826925), (437, 0.03305111041580294), (468, 0.06399408613376326), (493, 0.040698628220501666), (571, 0.02646515536472056), (744, 0.08158474710668752), (879, 0.07285443999368267), (1006, 0.10881160811928077), (1015, 0.08427214005962949), (1019, 0.0891626469605082), (1081, 0.13443880177149273), (1085, 0.042678361393408346), (1165, 0.06420161769596215), (1468, 0.12411523651684894), (1921, 0.14015049752912329), (2083, 0.058064692691376735), (2450, 0.15837415400038504), (2556, 0.08638641296168773), (2682, 0.07958236268399015), (3015, 0.09437268184930808), (3096, 0.14954007018952994), (3473, 0.12411523651684894), (3653, 0.18161059221407863), (3882, 0.10389483164289876), (4150, 0.12340182072796659), (5420, 0.14623015768310524), (8911, 0.14188763871697904), (9862, 0.17823563074893484), (10078, 0.3027008036977544), (10568, 0.11925446602753188), (12927, 0.15464760463940938), (13233, 0.27468592323212293), (13294, 0.17248560180296532), (13557, 0.15837415400038504), (13857, 0.14783734828990874), (17481, 0.2140659627928773), (17649, 0.14839375517082629), (20315, 0.12797587154135895), (20598, 0.17381858884350726), (25858, 0.18543680795035408), (26411, 0.19809710749748466), (28628, 0.15262412505441553), (37299, 0.18543680795035408), (52901, 0.18756030178700833), (85235, 0.20147206896262843), (87518, 0.15837415400038504)]\n"
     ]
    }
   ],
   "source": [
    "tfidf_model = True\n",
    "reflesh == False\n",
    "if tfidf_model == True:\n",
    "    if isfile(models_path + '/news20.tfidf_model') and (reflesh == False):\n",
    "        tfidf = TfidfModel.load(models_path + '/news20.tfidf_model')\n",
    "    else:\n",
    "        tfidf = TfidfModel(bow_x_train)\n",
    "        tfidf.save(models_path + '/news20.tfidf_model')\n",
    "    #测试\n",
    "    print(tfidf[bow_x_val[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tfidf_model == True:\n",
    "    tfidf_train_x = [tfidf[x] for x in bow_train_x]\n",
    "    tfidf_test_x  = [tfidf[x] for x in bow_test_x]\n",
    "    \n",
    "    tfidf_x_train = [tfidf[x] for x in bow_x_train]\n",
    "    tfidf_x_val = [tfidf[x] for x in bow_x_val]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.010940845536619908), (4, 0.008511887623988256), (5, 0.014390434101620948), (8, 0.006661137953839601), (9, 0.020513008912391825), (19, 0.030799162447278598), (26, 0.011136238424417615), (30, 0.036601814684047086), (31, 0.015793091033235465), (34, 0.011189180508555262), (36, 0.021108880153382078), (37, 0.02099154841435474), (40, 2.440813497992095e-05), (48, 0.011261578272341836), (49, 0.0003394881509476679), (61, 0.01981863637054047), (63, 0.0160941880892295), (67, 0.01568195232009917), (71, 0.017196979781760983), (76, 0.03264534547884898), (81, 0.02529902260822818), (83, 0.007539568327453713), (90, 0.014264104644020734), (93, 0.02013183359476064), (104, 0.012283147401450676), (105, 0.018992881357315108), (108, 0.007115488008964714), (114, 0.0655625994910856), (116, 0.018021913387041025), (123, 0.02581394714129518), (148, 0.0031758150978317763), (157, 0.01712119694488178), (166, 0.013334485561103039), (169, 0.01316469778884457), (173, 0.022209898014220752), (180, 0.011262562902480601), (183, 0.029392505725773888), (188, 0.004650273923590951), (190, 0.01590114445448598), (209, 0.022510990631503375), (212, 0.021407423226959262), (260, 0.02137040092269237), (264, 0.01936125062566572), (265, 0.023780035665242966), (281, 0.00587056223915288), (284, 0.015860029277274194), (285, 0.021879986777205197), (305, 0.025808058739153632), (318, 0.01511564936346458), (328, 0.01262485683141247), (331, 0.023167370159274107), (335, 0.03302068396770442), (346, 0.01887934359938942), (348, 0.025930673931706973), (350, 0.11418378375876587), (368, 0.04565813475633585), (396, 0.021950335731683555), (402, 0.01682147269579011), (409, 0.015076811447836075), (421, 0.01952816690798249), (430, 0.00921512780027297), (436, 0.011631046952957412), (437, 0.009786632170928412), (469, 0.01667599811234954), (488, 0.024179985686440468), (491, 0.014099044323419274), (492, 0.01913473857817513), (493, 0.024102216188474777), (516, 0.014410903683026924), (517, 0.024353918627031977), (518, 0.04483451327560176), (528, 0.00642600156429045), (529, 0.03083458895961274), (548, 0.029400826548418885), (569, 0.03127166261945317), (571, 0.007836491350594782), (572, 0.022371781328767774), (621, 0.005735188581887507), (622, 0.022637779206425363), (645, 0.0160941880892295), (759, 0.026486230418718983), (814, 0.0647284503177838), (835, 0.013325464148900162), (837, 0.020009830692500896), (860, 0.018180213452273414), (894, 0.025078657271174715), (909, 0.03362577227038764), (967, 0.02257049386270416), (1030, 0.03848563254934119), (1130, 0.015485515133303335), (1165, 0.03802096869148023), (1200, 0.026900197269283325), (1295, 0.013783954457475607), (1297, 0.01829280744892393), (1379, 0.08067060350997411), (1436, 0.020776974573789117), (1521, 0.04538797722290659), (1541, 0.0286550746129275), (1798, 0.05736515502687066), (1857, 0.08250690955994885), (1864, 0.03839860986466074), (1973, 0.043299627381267976), (1975, 0.04101443510430699), (1985, 0.035433174337485114), (2137, 0.04645017800695629), (2228, 0.03021849794835685), (2251, 0.034563679989694805), (2280, 0.02305309722147659), (2287, 0.02868257751343533), (2327, 0.032136586879037775), (2391, 0.041752695085777405), (2473, 0.03668026492353808), (2480, 0.034843726446400144), (2520, 0.019833265392801253), (2531, 0.03940878536078878), (2616, 0.013535422855500887), (2782, 0.045192919497287536), (2814, 0.02131443996893892), (2935, 0.02559866647554329), (2950, 0.025930673931706973), (3056, 0.042147400549233216), (3135, 0.02026132027637221), (3143, 0.014020391270003606), (3379, 0.051821560565573584), (3494, 0.03291633661251052), (3514, 0.02759795771143591), (3641, 0.0263386677884467), (3658, 0.04689553758056647), (3685, 0.044279712340920215), (3700, 0.025503439467215557), (3759, 0.05832861132314316), (3769, 0.026855658388124162), (3774, 0.034618954695789454), (4170, 0.033240040512027946), (4191, 0.027525542159097048), (4202, 0.023534727855693358), (4609, 0.018374156464972598), (4635, 0.19955490735658407), (4776, 0.032136586879037775), (4961, 0.02377846849109979), (5725, 0.04284900977229891), (6093, 0.04299657240257119), (6405, 0.060110024802885784), (6543, 0.04201378145106777), (6683, 0.03264837842585363), (6847, 0.04066807528377799), (7106, 0.0471345572562339), (7757, 0.04738050599506919), (7853, 0.04256136739979756), (8077, 0.04763379756203688), (8403, 0.03912114298828743), (9425, 0.07786931986965123), (10148, 0.09117524992148669), (11197, 0.06079005432477243), (11985, 0.09158416789515259), (12178, 0.048164260119732814), (12688, 0.0523178992903408), (13265, 0.050335761992076594), (13884, 0.050696864887344574), (14145, 0.03467459186156237), (14159, 0.09476101199013838), (14832, 0.09243280846249306), (15066, 0.04844246987605703), (15324, 0.038142772269953766), (15469, 0.051468727437002815), (15519, 0.04600159181130768), (15930, 0.0543235723523165), (16507, 0.04933641564531582), (18650, 0.04579208394757629), (19492, 0.048730112248558395), (21234, 0.0596570888798462), (21253, 0.11107546223803226), (21951, 0.045002245464546904), (23861, 0.045002245464546904), (24334, 0.27862078088448794), (25176, 0.053775986403586724), (27013, 0.0596570888798462), (28690, 0.22215092447606452), (30353, 0.06797906942085505), (31165, 0.3373011868100164), (33113, 0.05776379676382665), (33630, 0.04965662864275666), (34298, 0.06079005432477243), (43703, 0.11107546223803226), (46458, 0.06364489924008612), (46674, 0.053775986403586724), (46701, 0.0596570888798462), (80569, 0.1359581388417101), (80570, 0.06797906942085505), (81175, 0.5243055308488455), (82010, 0.06797906942085505), (82011, 0.06553819135610568), (82012, 0.06364489924008612), (82013, 0.07141929383236516), (82014, 0.06364489924008612), (82015, 0.06797906942085505), (82016, 0.06553819135610568), (82017, 0.07141929383236516), (82020, 0.07141929383236516), (82021, 0.07141929383236516), (82023, 0.07141929383236516), (83299, 0.07730039630862463), (83300, 0.20393720826256512), (83301, 0.07730039630862463), (83302, 0.07141929383236516), (83303, 0.07141929383236516), (83304, 0.14283858766473032)]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_x_val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf相似性检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.9999998211860657), (505, 0.5786049962043762), (6010, 0.5432599782943726), (594, 0.5040692090988159), (4129, 0.4994984567165375), (3456, 0.4873291254043579), (5684, 0.43764516711235046), (7043, 0.25720927119255066), (1802, 0.23871898651123047), (2346, 0.2328898012638092)]\n"
     ]
    }
   ],
   "source": [
    "if tfidf_model == True:\n",
    "    #TFIDF相似性检测\n",
    "    nb_feature = len(dictionary)\n",
    "    from gensim.similarities.docsim import Similarity\n",
    "    index_tfidf = Similarity('temp/similty_idx', \n",
    "                             tfidf_x_train, \n",
    "                             num_features=nb_feature)\n",
    "\n",
    "    index_tfidf.num_best = 10\n",
    "    print (index_tfidf[tfidf_x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, isdir\n",
    "import tarfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "text8_path = 'text8'\n",
    "\n",
    "if not isdir(text8_path):\n",
    "    with ZipFile('text8.zip') as zf:\n",
    "        zf.extractall(text8_path)\n",
    "        zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import warnings\n",
    "import pandas\n",
    "from gensim import corpora,models,similarities\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from os import mkdir\n",
    "\n",
    "max_vocab_size = 50000\n",
    "reflesh = False\n",
    "\n",
    "def word2vec_model(size):\n",
    "    model_file = models_path + \"/word2vec\" + str(size) + \".md\"\n",
    "    if isfile(model_file) and (reflesh == False):\n",
    "        model = Word2Vec.load(model_file)\n",
    "    else:\n",
    "        model = Word2Vec(new_train_x, size=size, window=5, max_vocab_size = max_vocab_size,\n",
    "                         min_count=5, workers=4)\n",
    "        #检查并创建模型目录\n",
    "        assert isdir(models_path), mkdir(models_path)\n",
    "        #保存模型\n",
    "        model.save(model_file)\n",
    "    return model\n",
    "\n",
    "def word2vec_model2(size):\n",
    "    model_file = models_path + \"/word2vectext\" + str(size) + \".md\"\n",
    "    if isfile(model_file) and (reflesh == False):\n",
    "        model = Word2Vec.load(model_file)\n",
    "    else:\n",
    "        sentences = models.word2vec.Text8Corpus(text8_path + '/text8',  max_sentence_length=1000)\n",
    "        model = Word2Vec(sentences, size=size, window=5, max_vocab_size = max_vocab_size,\n",
    "                         min_count=5, workers=4)\n",
    "        #检查并创建模型目录\n",
    "        assert isdir(models_path), mkdir(models_path)\n",
    "        #保存模型\n",
    "        model.save(model_file)\n",
    "    return model\n",
    "\n",
    "model_word2vec = True\n",
    "#使用20组新闻样本训练的word2vec模型\n",
    "# if model_word2vec == True:\n",
    "#     model = word2vec_model(200)\n",
    "#     #使用模型实例200纬度\n",
    "#     model_word2vec200 = model.wv\n",
    "\n",
    "#使用text8训练word2vec模型\n",
    "if model_word2vec == True:\n",
    "    model = word2vec_model2(200)\n",
    "    #使用模型实例200纬度\n",
    "    model_word2vec200 = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec检测相似性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9704541   0.5189756   0.6744587   0.01087062  1.0679857  -0.52487886\n",
      " -0.64980954 -0.4533574   0.56106836  1.6472147   0.19555567 -1.9471778\n",
      "  0.7707421   2.0987034  -0.226309    0.5256508   1.255615    0.36819422\n",
      "  1.5916296   2.262886   -0.57896924  0.28168982  1.5931888  -1.4215323\n",
      "  0.20646362 -0.99576205  0.6164254  -0.6124262  -0.902483    1.3772614\n",
      " -1.01067     2.1639864  -0.04831955  0.39986974 -0.7240784  -1.1605207\n",
      " -0.91958696  0.77921253  1.3705611  -1.2327397   1.2102787  -0.45162696\n",
      "  1.6618432   0.4805663   0.42414418  0.9522331   0.05192119 -0.8744376\n",
      "  1.1857388   1.3332164   0.06589112  0.7241123  -0.43246517 -0.28568465\n",
      "  1.3812044   0.02615357  0.980446    2.20354     0.33822533  1.6895237\n",
      "  0.918058   -2.3457983   0.9654337  -2.1902282  -0.14903644 -0.30096808\n",
      "  1.0672555  -0.15213037  0.49626437  0.68427944 -1.0117623  -0.23785764\n",
      "  1.8554426  -2.0654829   0.12752692 -0.5877149   0.05665542 -0.12666978\n",
      "  0.04559718  0.24326645 -0.16387601 -1.2972243  -0.28819516 -0.17935216\n",
      " -0.07957463  0.03323144 -1.7044386   0.7731077  -0.60920954 -0.7671795\n",
      "  1.486634    0.42613894  0.23079652  0.04843001  1.0465313   0.2366205\n",
      "  1.1170292   0.04087196 -1.511157    1.6716243  -2.1377203   0.8251235\n",
      "  0.2022181  -1.405865    0.5144001  -0.703667   -3.062321    2.7533846\n",
      " -1.8972256   2.7769842   0.0770858  -0.4195515   2.710339   -1.4011029\n",
      "  1.2940513   0.95405763  0.8729124  -0.27672264 -0.5885101  -0.3628722\n",
      "  0.555199   -1.1266853  -0.74329674  0.7894892  -1.0779667  -0.6691988\n",
      " -1.1206346  -1.4344662  -0.59767115 -1.4505658  -0.04334407 -0.810603\n",
      " -0.28534332  1.1465747  -0.25355497  0.97279483  0.67465574  0.09050369\n",
      "  0.93782777  2.108977   -0.29757345 -0.01774171 -0.38008752  0.15908958\n",
      "  0.18737693 -0.2425408  -0.16299884 -0.46956226 -0.6224946   0.85877705\n",
      "  0.11948857  0.30281028  0.0364861   0.5839554  -0.76037633 -0.18534084\n",
      "  0.812533   -1.2385932  -0.38249055  1.8127209  -0.01172345  0.64581484\n",
      "  1.1557715  -0.6936992  -0.11233529 -1.047151   -2.1473825  -0.6309702\n",
      " -1.6109017   1.0420313  -2.366033    0.74262357 -0.67956877 -1.3064564\n",
      " -0.22891265 -0.29993117 -3.329228    0.353298   -0.48279658 -2.4089036\n",
      "  2.5889454   0.39640632 -0.40977103  0.8314463  -0.6409224  -0.3572018\n",
      " -1.4980345   0.58215314 -1.0654844   0.9823699   2.15884    -1.8032166\n",
      "  0.29152003  0.5280488   0.44231555 -1.970205   -1.4099967  -0.12375517\n",
      " -1.867537    0.06071753]\n",
      "The similarity of 'computer' and 'pc' is: 0.5194796315708924\n",
      "The similarity of 'computer' and 'hp' is: 0.38653433728798603\n"
     ]
    }
   ],
   "source": [
    "def model_test():\n",
    "    try:\n",
    "        print (model_word2vec200[\"computer\"])\n",
    "        \n",
    "        print(\"The similarity of 'computer' and 'pc' is:\",\n",
    "              model_word2vec200.similarity('computer', 'pc'))\n",
    "        \n",
    "        print(\"The similarity of 'computer' and 'hp' is:\",\n",
    "              model_word2vec200.similarity('computer', 'hp'))\n",
    "    except:\n",
    "        pass\n",
    "model_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用word2vec200词向量平均值表示文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.07998354, -0.72177639,  0.1048835 , -0.35636717, -0.08737985,\n",
      "        0.55299945,  0.16822696, -0.34368943, -0.21055081,  0.74284476,\n",
      "        0.48132571,  0.35950611, -0.28621166,  0.11023551,  0.12628022,\n",
      "       -0.24780488,  0.20211906, -0.34610975,  0.53338657, -0.00257856,\n",
      "       -0.17577102,  0.0736735 ,  0.45189972, -0.08615299, -0.11881772,\n",
      "        0.253095  ,  0.00663455,  0.29833392, -0.60384475,  0.33272245,\n",
      "        0.27389537,  0.07908884,  0.2600565 , -0.02438287,  0.41406304,\n",
      "        0.20309591,  0.34143777, -0.17014372,  0.30544874,  0.04337316,\n",
      "        0.1616682 , -0.18096707, -0.50642577,  0.06496233,  0.10249004,\n",
      "       -0.05091331,  0.11897509, -0.17966712,  0.22777721,  0.26097476,\n",
      "        0.23721102,  0.13545604,  0.17124161, -0.31660963,  0.08593192,\n",
      "        0.32362013, -0.18843826, -0.08942551,  0.07029901,  0.19118327,\n",
      "        0.18826906, -0.23097972,  0.42046281, -0.11845288, -0.24615203,\n",
      "       -0.16381095, -0.12393107, -0.47557833,  0.29429314, -0.08801867,\n",
      "        0.14530829,  0.11672844, -0.17937713,  0.10268761,  0.22484064,\n",
      "       -0.25383921,  0.00470365, -0.27702672, -0.09228824, -0.57430135,\n",
      "       -0.3575686 ,  0.04247578,  0.11106914, -0.07073875,  0.14212218,\n",
      "        0.14388464,  0.02007483, -0.01297921, -0.21168864,  0.27186671,\n",
      "       -0.40009543, -0.01281643,  0.03805744,  0.26182978, -0.1605295 ,\n",
      "        0.30458219,  0.16265099,  0.11498966, -0.16515563, -0.16387774,\n",
      "        0.05506989, -0.02265206,  0.29197943, -0.04758485, -0.43214457,\n",
      "       -0.23327477,  0.09766959, -0.14024955, -0.05950068, -0.10863258,\n",
      "        0.25738608,  0.11798886,  0.32705109, -0.07865665, -0.67971576,\n",
      "        0.532437  , -0.0225629 ,  0.27408495, -0.48500859,  0.38500517,\n",
      "        0.07258431,  0.01868187,  0.97636124,  0.10525015, -0.13222619,\n",
      "       -0.28076828, -0.24625969, -0.23280019, -0.32375846, -0.1843613 ,\n",
      "        0.33766427,  0.20427512, -0.05329108,  0.14547386, -0.226068  ,\n",
      "        0.01197957,  0.09667313, -0.07026117, -0.06009022,  0.17753227,\n",
      "       -0.19054597, -0.20529249,  0.0690133 ,  0.17041286, -0.12675703,\n",
      "       -0.27737218,  0.05945819, -0.23433841, -0.12962643,  0.21343177,\n",
      "        0.12179343,  0.03200818, -0.03402417, -0.36858438, -0.03318531,\n",
      "        0.22655302, -0.29219229, -0.13772948, -0.43996966, -0.17696328,\n",
      "       -0.11889663, -0.03585515, -0.00705179,  0.37406889,  0.0661228 ,\n",
      "       -0.21104574,  0.0106496 , -0.02335904,  0.19808215, -0.16926289,\n",
      "        0.04364021, -0.18875387,  0.43690009, -0.19846581,  0.20428444,\n",
      "        0.17830644,  0.39238816,  0.48343029, -0.3043406 , -0.04453037,\n",
      "       -0.02062988, -0.05155548, -0.32818007,  0.40451452, -0.2363971 ,\n",
      "       -0.15818016,  0.03721275,  0.07937957, -0.11588226,  0.09149177,\n",
      "       -0.5594538 , -0.1148376 ,  0.31498836,  0.24364972, -0.04098329,\n",
      "       -0.19801864, -0.03108783, -0.43634037,  0.46410962, -0.3161271 ])]\n",
      "[array([ 0.13932871, -0.62414618,  0.0713762 , -0.10690596, -0.22164377,\n",
      "        0.48974335,  0.23579036, -0.26366712, -0.12549854,  0.63346263,\n",
      "        0.50206363,  0.61910456, -0.29757536,  0.16139483,  0.16310371,\n",
      "       -0.20761335,  0.2444323 , -0.27398224,  0.46890192, -0.17632375,\n",
      "       -0.15129475,  0.00410943,  0.16926109, -0.28574165, -0.17912231,\n",
      "        0.21190991, -0.17269226,  0.22172348, -0.56708889,  0.07915262,\n",
      "        0.07014584,  0.10829496,  0.00433343,  0.25224661,  0.35574212,\n",
      "        0.28172959,  0.19343194, -0.08612716,  0.3196491 , -0.30946031,\n",
      "        0.23296641, -0.02702448, -0.28585759,  0.07504965,  0.14478668,\n",
      "        0.06248895, -0.11783462, -0.03333551,  0.26139231,  0.04461127,\n",
      "        0.29085974, -0.10682179,  0.06151402, -0.19886178,  0.29776495,\n",
      "       -0.03963156, -0.01872146, -0.06945732,  0.08133494,  0.12462477,\n",
      "        0.35247841, -0.23198685,  0.09270905, -0.25270463,  0.03631133,\n",
      "       -0.01737585,  0.07089675, -0.26856971, -0.02908573,  0.07903373,\n",
      "        0.19082024,  0.19314159, -0.43036773,  0.02660749,  0.32775806,\n",
      "       -0.16722013, -0.03782368,  0.08838093,  0.077966  , -0.30653816,\n",
      "       -0.02597339,  0.07739349,  0.17391055, -0.0672467 ,  0.06463724,\n",
      "       -0.04397819, -0.05989752,  0.04503254, -0.27788556,  0.24342659,\n",
      "       -0.15756041,  0.07535224,  0.10686719,  0.47152066, -0.40898488,\n",
      "        0.43329424,  0.12913444, -0.03670821, -0.18229411, -0.22516125,\n",
      "        0.14859358, -0.14730982,  0.23470513, -0.15686409, -0.27132365,\n",
      "       -0.24281335,  0.01517238, -0.28778414, -0.2359628 , -0.1502081 ,\n",
      "        0.30138556,  0.17779377,  0.44270636,  0.13134319, -0.29229049,\n",
      "        0.42328552, -0.01185501, -0.10531821, -0.33403461,  0.3630791 ,\n",
      "        0.02260418, -0.00159826,  0.64768306,  0.10072445, -0.29783882,\n",
      "       -0.35354964, -0.22305865, -0.10705105, -0.11245648,  0.0120124 ,\n",
      "        0.19510882,  0.32205237, -0.16694513,  0.33971769, -0.25991724,\n",
      "        0.25944695,  0.15070225,  0.13774865,  0.25408598,  0.13376825,\n",
      "       -0.16246472, -0.05027627,  0.02895391,  0.1967345 , -0.05647198,\n",
      "       -0.27515859, -0.17141104,  0.07749865, -0.14066655,  0.23000279,\n",
      "        0.16445611, -0.13189869, -0.08993967, -0.27952773, -0.03992353,\n",
      "        0.31114254, -0.1490922 , -0.18604516, -0.23973314, -0.39740309,\n",
      "       -0.32177366, -0.24187725,  0.28014414,  0.32602834,  0.18526434,\n",
      "       -0.31768241, -0.17186962, -0.24918709,  0.14710155, -0.14787707,\n",
      "       -0.43530131,  0.17600996,  0.17997988, -0.19742047,  0.26452595,\n",
      "        0.14973818,  0.2865384 ,  0.22282459, -0.25298702, -0.33388886,\n",
      "        0.12310875,  0.23004197, -0.45662661,  0.35498199, -0.28190142,\n",
      "        0.02927333, -0.24783068,  0.08145312, -0.25864704,  0.17361995,\n",
      "       -0.28802212, -0.15285533,  0.24207764,  0.48864538,  0.16838083,\n",
      "       -0.12326467, -0.12652175, -0.44748393,  0.31097736, -0.24637222])]\n",
      "[array([-0.15674765, -0.26034779,  0.04047547, -0.19653704, -0.40083349,\n",
      "        0.03459005,  0.11607342, -0.23139549,  0.07669094,  0.57723082,\n",
      "        0.33527912, -0.02055698, -0.18449075,  0.07955719,  0.17602868,\n",
      "       -0.12520186, -0.19986269, -0.37109579,  0.33054785, -0.07143626,\n",
      "       -0.27960976,  0.1731061 ,  0.4387353 , -0.38157298, -0.23935083,\n",
      "        0.15954128,  0.02972306,  0.11970688, -0.30074082,  0.15412656,\n",
      "       -0.2704905 ,  0.15902491,  0.21035981,  0.16929167, -0.11913235,\n",
      "       -0.08938796,  0.18973372, -0.00235795,  0.2671514 , -0.40258794,\n",
      "        0.18907166,  0.01881954,  0.07425242,  0.25763267,  0.21586261,\n",
      "        0.02949166, -0.19360863, -0.09348828, -0.03292469, -0.06913619,\n",
      "        0.07063478,  0.05100286,  0.22906557, -0.18745551,  0.3069851 ,\n",
      "       -0.26997043,  0.18350903, -0.09163815,  0.04663099, -0.07127632,\n",
      "        0.10467101, -0.37083688, -0.01943402, -0.05359421, -0.08531488,\n",
      "        0.09654373, -0.04462035, -0.22298918, -0.04308935,  0.34273387,\n",
      "       -0.11213178,  0.07342904, -0.05291486,  0.03377107,  0.00425896,\n",
      "       -0.16921419, -0.08109885,  0.12642979,  0.26606371, -0.18808553,\n",
      "       -0.00208033,  0.05812639,  0.29522665, -0.04669964,  0.20420133,\n",
      "       -0.12037272, -0.13965985, -0.21689659,  0.11589004, -0.03762845,\n",
      "        0.25364481,  0.06947591, -0.10197004,  0.32811309, -0.37436333,\n",
      "        0.200332  ,  0.11112447,  0.02910588, -0.16945038,  0.11892656,\n",
      "        0.07483094, -0.2735406 ,  0.33383962, -0.13943795, -0.26741894,\n",
      "       -0.30896149, -0.06098055,  0.11892959,  0.09982111,  0.0884151 ,\n",
      "        0.07396956,  0.02602184,  0.3143754 , -0.08097872, -0.12341004,\n",
      "        0.44053514, -0.0566029 , -0.08347217, -0.07626567,  0.02867003,\n",
      "       -0.11253047,  0.07423484,  0.37004174,  0.24287162, -0.26587329,\n",
      "       -0.14838351, -0.01101975, -0.03138011, -0.22257814, -0.07591336,\n",
      "       -0.17016544,  0.53485871, -0.17854727,  0.20215361,  0.00451086,\n",
      "        0.40176476,  0.2984159 , -0.07086628,  0.5805078 ,  0.18515084,\n",
      "       -0.07799231, -0.28922315,  0.21743361,  0.04150421, -0.06648292,\n",
      "        0.00903176, -0.23441376,  0.00423843, -0.26561715,  0.16763848,\n",
      "        0.27657504, -0.19661274, -0.24446695, -0.00796433, -0.15226213,\n",
      "       -0.04451796, -0.22751249,  0.16857149, -0.37674753, -0.10593812,\n",
      "        0.01732286, -0.08518371,  0.09915346,  0.07410969,  0.3918041 ,\n",
      "       -0.14348599,  0.03902851, -0.06062824, -0.11519485,  0.00128574,\n",
      "       -0.4977204 ,  0.34899615,  0.01185047, -0.3046922 ,  0.21882619,\n",
      "        0.30110503, -0.14629907, -0.1711084 , -0.20603047, -0.21108973,\n",
      "        0.02172574, -0.03099859, -0.20474993,  0.4370732 , -0.29111559,\n",
      "        0.02137442, -0.21857544,  0.15363499, -0.09357356,  0.26987096,\n",
      "        0.03857257, -0.3757436 ,  0.23854347,  0.17321623,  0.14691349,\n",
      "        0.02332604, -0.30410177, -0.4661723 , -0.07581346, -0.18890221])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "def average_word_vec(doc, size):\n",
    "    num_word = 0\n",
    "    word_vec_model = globals()[\"model_word2vec\" + str(size)]\n",
    "    sum_vec = np.zeros(len(word_vec_model['test'])) \n",
    "    for word in doc:\n",
    "        try:\n",
    "            word_vec = word_vec_model[word]\n",
    "            num_word += 1\n",
    "            sum_vec = sum_vec + word_vec\n",
    "        except: pass\n",
    "    return sum_vec/num_word\n",
    "\n",
    "average_word_vec200 = True\n",
    "if average_word_vec200 == True:\n",
    "\n",
    "#     生成文档向量\n",
    "    x_train_average_word_vec200= [average_word_vec(x, 200) for x in x_train]\n",
    "    print (x_train_average_word_vec200[:1])\n",
    "    \n",
    "    x_val_average_word_vec200 = [ average_word_vec(x, 200) for x in x_val]\n",
    "    print (x_val_average_word_vec200[:1])\n",
    "    \n",
    "    test_x_average_word_vec200 = [ average_word_vec(x, 200) for x in new_test_x]\n",
    "    print (test_x_average_word_vec200[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成doc2vec文档向量\n",
    "    \n",
    "    文档标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['from', 'dbd', 'urartu', 'sdpa', 'org', 'david', 'davidian', 'subject', 'treatment', 'armenians', 'azerbaijan', 'summary', 'prelude', 'events', 'today', 'organization', 's', 'd', 'p', 'a', 'center', 'regional', 'studies', 'lines', 'deposition', 'vitaly', 'nikolayevich', 'danielian', 'born', 'attended', 'grade', 'middle', 'school', 'no', 'resident', 'at', 'building', 'apartment', 'microdistrict', 'no', 'sumgait', 'azerbaijan', 'really', 'people', 'town', 'didn', 't', 'know', 'what', 'was', 'happening', 'on', 'february', 'i', 'came', 'home', 'from', 'school', 'at', 'o', 'clock', 'being', 'excused', 'leave', 'before', 'last', 'period', 'order', 'go', 'baku', 'when', 'we', 'left', 'everything', 'town', 'was', 'fine', 'life', 'was', 'same', 'as', 'usual', 'a', 'few', 'groups', 'people', 'were', 'discussing', 'things', 'soccer', 'other', 'things', 'then', 'we', 'got', 'on', 'sumgait', 'bus', 'bound', 'baku', 'my', 'first', 'cousin', 's', 'birthday', 'my', 'father', 'my', 'mother', 'i', 'we', 'spent', 'day', 'baku', 'on', 'somewhere', 'around', 'p', 'm', 'we', 'got', 'on', 'bus', 'home', 'figuring', 'that', 'i', 'd', 'have', 'enough', 'time', 'do', 'my', 'homework', 'next', 'day', 'when', 'we', 'were', 'entering', 'town', 'near', 'story', 'high', 'rises', 'our', 'bus', 'was', 'stopped', 'by', 'a', 'very', 'large', 'crowd', 'crowd', 'demanded', 'that', 'armenians', 'get', 'off', 'bus', 'driver', 'says', 'that', 'there', 'are', 'no', 'armenians', 'on', 'board', 'then', 'everyone', 'on', 'bus', 'begins', 'shout', 'that', 'there', 'are', 'no', 'armenians', 'on', 'board', 'group', 'comes', 'up', 'doors', 'bus', 'has', 'people', 'get', 'out', 'one', 'by', 'one', 'not', 'checking', 'passports', 'just', 'going', 'by', 'way', 'people', 'look', 'we', 'get', 'off', 'bus', 'but', 'are', 'not', 'taken', 'armenians', 'we', 'set', 'out', 'direction', 'home', 'at', 'first', 'we', 'were', 'going', 'go', 'into', 'an', 'old', 'building', 'where', 'we', 'knew', 'there', 'd', 'be', 'a', 'place', 'hide', 'but', 'whole', 'road', 'was', 'packed', 'with', 'groups', 'people', 'all', 'way', 'from', 'block', 'microdistrict', 'these', 'groups', 'were', 'emptying', 'people', 's', 'pockets', 'checking', 'passports', 'people', 'who', 'didn', 't', 'have', 'passports', 'with', 'them', 'were', 'beaten', 'as', 'well', 'then', 'we', 'decided', 'go', 'home', 'instead', 'near', 'story', 'high', 'rises', 'i', 'saw', 'burning', 'cars', 'a', 'great', 'many', 'people', 'standing', 'around', 'driveways', 'yelling', 'death', 'armenians', 'was', 'written', 'on', 'cars', 'when', 'we', 'came', 'into', 'courtyard', 'we', 'live', 'an', 'l', 'shaped', 'building', 'it', 'was', 'still', 'quiet', 'we', 'went', 'on', 'upstairs', 'but', 'didn', 't', 'turn', 'on', 'any', 'lights', 'we', 'tried', 'call', 'baku', 'warn', 'our', 'relatives', 'who', 'were', 'due', 'arrive', 'on', 'wednesday', 'not', 'come', 'then', 'there', 'was', 'a', 'knock', 'at', 'door', 'it', 'was', 'our', 'neighbors', 'who', 'advised', 'us', 'come', 'down', 'stay', 'at', 'their', 'place', 'we', 'went', 'down', 'their', 'place', 'they', 'led', 'us', 'basement', 'they', 'live', 'on', 'first', 'floor', 'have', 'a', 'basement', 'which', 'you', 'enter', 'across', 'balcony', 'we', 'sat', 'basement', 'while', 'an', 'armenian', 'woman', 'was', 'beaten', 'she', 'ran', 'away', 'naked', 'our', 'neighbors', 'daughter', 'said', 'that', 'that', 's', 'right', 'that', 's', 'what', 'armenians', 'deserve', 'because', 'stepanakert', 'allegedly', 'people', 'were', 'being', 'killed', 'girls', 'from', 'agdam', 'had', 'been', 'raped', 'we', 'didn', 't', 'stay', 'very', 'long', 'basement', 'we', 'tried', 'support', 'one', 'another', 'as', 'best', 'we', 'could', 'looking', 'out', 'small', 'window', 'with', 'iron', 'grating', 'papa', 'watched', 'said', 'things', 'now', 'then', 'he', 'said', 'that', 'there', 'was', 'a', 'fire', 'near', 'building', 'probably', 'a', 'car', 'on', 'fire', 'then', 'one', 'groups', 'approached', 'our', 'driveway', 'demanded', 'that', 'they', 'be', 'shown', 'apartments', 'where', 'armenians', 'lived', 'neighbors', 'said', 'that', 'there', 'weren', 't', 'any', 'armenians', 'here', 'group', 'set', 'out', 'other', 'wing', 'building', 'they', 'appeared', 'from', 'side', 'building', 'where', 'i', 'later', 'found', 'out', 'a', 'woman', 'had', 'been', 'murdered', 'woman', 'who', 'ran', 'away', 'naked', 'died', 'yuri', 'avakian', 'was', 'killed', 'too', 'when', 'crowd', 'left', 'neighbors', 'said', 'that', 'it', 'was', 'all', 'over', 'we', 'could', 'go', 'home', 'we', 'went', 'back', 'up', 'our', 'place', 'again', 'didn', 't', 'turn', 'on', 'light', 'we', 'started', 'gather', 'up', 'our', 'things', 'order', 'leave', 'sumgait', 'a', 'while', 'we', 'tried', 'call', 'a', 'relative', 'who', 'lived', 'sumgait', 'but', 'there', 'was', 'no', 'answer', 'we', 'decided', 'she', 'had', 'already', 'left', 'we', 'sat', 'at', 'home', 'phone', 'rang', 'caller', 'asked', 'speak', 'with', 'my', 'father', 'i', 'called', 'him', 'phone', 'it', 'was', 'jeykhun', 'mamedov', 'from', 'my', 'father', 's', 'work', 'brigade', 'he', 'said', 'he', 'was', 'disgusted', 'by', 'what', 'was', 'happening', 'our', 'town', 'he', 'asked', 'our', 'address', 'promised', 'get', 'a', 'car', 'help', 'us', 'get', 'out', 'city', 'be', 'quite', 'honest', 'papa', 'didn', 't', 'want', 'give', 'him', 'our', 'address', 'but', 'my', 'mother', 'got', 'on', 'phone', 'told', 'him', 'some', 'minutes', 'after', 'call', 'a', 'crowd', 'ran', 'into', 'our', 'entryway', 'bursting', 'into', 'building', 'they', 'broke', 'down', 'door', 'came', 'into', 'apartment', 'they', 'came', 'straight', 'our', 'apartment', 'they', 'knew', 'exactly', 'where', 'armenians', 'were', 'they', 'came', 'into', 'our', 'place', 'we', 'tried', 'resist', 'but', 'there', 'was', 'nothing', 'we', 'could', 'do', 'one', 'them', 'took', 'my', 'parents', 'passports', 'began', 'read', 'them', 'he', 'read', 'surname', 'danielian', 'turned', 'page', 'read', 'armenian', 'that', 'alone', 'was', 'enough', 'doom', 'us', 'he', 'said', 'that', 'we', 'should', 'be', 'moved', 'quickly', 'out', 'into', 'courtyard', 'where', 'they', 'would', 'have', 'done', 'with', 'us', 'another', 'standing', 'next', 'him', 'pushed', 'some', 'keys', 'on', 'piano', 'said', 'your', 'death', 'has', 'tolled', 'they', 'had', 'knives', 'steel', 'truncheons', 'i', 'had', 'a', 'knife', 'my', 'hand', 'unfortunately', 'i', 'didn', 't', 'use', 'it', 'i', 'just', 'knew', 'that', 'if', 'i', 'didn', 't', 'give', 'up', 'knife', 'things', 'would', 'be', 'much', 'worse', 'they', 'struck', 'my', 'parents', 'said', 'that', 'i', 'should', 'put', 'knife', 'on', 'piano', 'then', 'one', 'them', 'commanded', 'that', 'we', 'be', 'taken', 'outside', 'one', 'person', 'was', 'giving', 'orders', 'when', 'we', 'were', 'taken', 'outdoors', 'i', 'went', 'middle', 'my', 'mother', 'was', 'behind', 'me', 'someone', 'started', 'push', 'her', 'so', 'she', 'd', 'walk', 'faster', 'i', 'let', 'her', 'go', 'ahead', 'me', 'fell', 'behind', 'her', 'when', 'he', 'tried', 'push', 'me', 'i', 'hit', 'him', 'at', 'that', 'moment', 'they', 'began', 'beating', 'my', 'parents', 'i', 'realized', 'that', 'resistance', 'was', 'completely', 'useless', 'we', 'are', 'taken', 'out', 'into', 'courtyard', 'neighbors', 'are', 'standing', 'on', 'their', 'balconies', 'see', 'what', 'will', 'happen', 'next', 'crowd', 'surrounds', 'us', 'at', 'first', 'they', 'strike', 'me', 'i', 'm', 'knocked', 'out', 'when', 'i', 'come', 'they', 'beat', 'me', 'again', 'i', 'lose', 'consciousness', 'often', 'i', 'don', 't', 'see', 'or', 'hear', 'my', 'parents', 'since', 'i', 'was', 'first', 'one', 'hit', 'was', 'out', 'cold', 'when', 'i', 'come', 'i', 'try', 'pick', 'them', 'up', 'they', 'are', 'lying', 'next', 'me', 'crowd', 'is', 'gone', 'only', 'people', 'around', 'are', 'watching', 'from', 'their', 'balconies', 'that', 's', 'it', 'i', 'try', 'pick', 'them', 'up', 'but', 'can', 't', 'my', 'left', 'arm', 'is', 'broken', 'i', 'start', 'toward', 'drive', 'wanting', 'tell', 'neighbors', 'call', 'an', 'ambulance', 'bodies', 'my', 'parents', 'are', 'still', 'warm', 'we', 'were', 'attacked', 'at', 'around', 'o', 'clock', 'i', 'regain', 'consciousness', 'at', 'about', 'try', 'make', 'it', 'up', 'stairs', 'home', 'when', 'i', 'knock', 'at', 'neighbors', 'door', 'they', 'push', 'me', 'back', 'tell', 'me', 'go', 'away', 'i', 'go', 'up', 'third', 'floor', 'our', 'neighbor', 'puts', 'a', 'damp', 'cloth', 'on', 'my', 'head', 'says', 'she', 'will', 'call', 'an', 'ambulance', 'she', 'sends', 'her', 'son', 'off', 'one', 'takes', 'me', 'our', 'apartment', 'i', 'often', 'look', 'out', 'window', 'see', 'if', 'ambulance', 'has', 'arrived', 'but', 'i', 'can', 't', 'see', 'very', 'far', 'as', 'a', 'result', 'blows', 'it', 'seems', 'that', 'my', 'parents', 'have', 'already', 'been', 'taken', 'away', 'then', 'i', 'calm', 'down', 'try', 'convince', 'myself', 'that', 'they', 'have', 'been', 'taken', 'away', 'everything', 'will', 'be', 'ok', 'but', 'they', 'were', 'still', 'there', 'later', 'at', 'morning', 'as', 'i', 'found', 'out', 'ambulance', 'picked', 'them', 'up', 'but', 'they', 'were', 'already', 'dead', 'if', 'they', 'received', 'attention', 'on', 'time', 'it', 'is', 'possible', 'they', 'would', 'still', 'be', 'alive', 'later', 'around', 'o', 'clock', 'on', 'policemen', 'civilian', 'clothing', 'come', 'our', 'house', 'with', 'some', 'assistants', 'they', 'call', 'an', 'ambulance', 'minutes', 'later', 'it', 'arrives', 'i', 'am', 'taken', 'sumgait', 'emergency', 'hospital', 'there', 'they', 'stitch', 'wounds', 'on', 'my', 'head', 'rebind', 'my', 'arm', 'at', 'o', 'clock', 'i', 'other', 'armenians', 'who', 'are', 'hospital', 'are', 'sent', 'by', 'ambulance', 'baku', 'my', 'ward', 'at', 'sumgait', 'hospital', 'there', 'were', 'five', 'people', 'all', 'them', 'armenians', 'hospital', 'was', 'nearly', 'overflowing', 'with', 'armenians', 'only', 'azerbaijanis', 'there', 'were', 'those', 'whose', 'car', 'had', 'flipped', 'over', 'before', 'events', 'before', 'then', 'i', 'was', 'semashko', 'hospital', 'baku', 'i', 'was', 'there', 'days', 'when', 'i', 'was', 'released', 'on', 'day', 'i', 'found', 'out', 'that', 'my', 'parents', 'were', 'dead', 'at', 'first', 'they', 'told', 'me', 'that', 'they', 'were', 'moscow', 'being', 'treated', 'but', 'later', 'i', 'found', 'out', 'that', 'they', 'were', 'dead', 'my', 'father', 's', 'older', 'brother', 'told', 'me', 'my', 'father', 's', 'name', 'was', 'nikolai', 'artemovich', 'danielian', 'he', 'was', 'born', 'my', 'mother', 'born', 'was', 'seda', 'osipovna', 'danielian', 'papa', 'worked', 'at', 'pmk', 'leader', 'roofing', 'brigade', 'mamma', 'was', 'a', 'compressor', 'operator', 'they', 'were', 'also', 'beaten', 'on', 'head', 'coroner', 's', 'report', 'stated', 'that', 'their', 'heads', 'were', 'smashed', 'open', 'bled', 'profusely', 'at', 'confrontation', 'i', 'met', 'jeykhun', 'mamedov', 'who', 'had', 'called', 'as', 'it', 'turned', 'out', 'later', 'he', 'had', 'been', 'one', 'who', 'tipped', 'crowd', 'off', 'he', 'had', 'called', 'specifically', 'find', 'out', 'if', 'we', 'were', 'at', 'home', 'find', 'out', 'exact', 'address', 'dispatch', 'group', 'he', 'knew', 'phone', 'number', 'but', 'didn', 't', 'know', 'address', 'before', 'events', 'i', 'had', 'never', 'seen', 'him', 'but', 'had', 'often', 'spoken', 'with', 'him', 'on', 'phone', 'when', 'he', 'would', 'ask', 'speak', 'with', 'my', 'father', 'i', 'knew', 'him', 'by', 'name', 'he', 'denies', 'that', 'i', 'was', 'one', 'who', 'answered', 'phone', 'saying', 'that', 'my', 'father', 'answered', 'it', 'he', 'denies', 'that', 'he', 'called', 'from', 'a', 'public', 'phone', 'saying', 'that', 'he', 'called', 'from', 'home', 'which', 'also', 'isn', 't', 'true', 'i', 'heard', 'noise', 'sounds', 'automobiles', 'as', 'i', 'later', 'found', 'out', 'earlier', 'he', 'had', 'been', 'convicted', 'but', 'had', 'never', 'served', 'any', 'time', 'he', 'had', 'received', 'a', 'suspended', 'sentence', 'he', 'was', 'about', 'years', 'old', 'i', 'don', 't', 'know', 'if', 'he', 'has', 'since', 'confessed', 'or', 'not', 'i', 'am', 'sure', 'that', 'he', 'was', 'one', 'who', 'tipped', 'crowd', 'off', 'one', 'hundred', 'percent', 'sure', 'my', 'parents', 'were', 'from', 'karabagh', 'father', 'was', 'from', 'village', 'badar', 'was', 'two', 'years', 'old', 'when', 'his', 'family', 'moved', 'baku', 'where', 'his', 'elder', 'brothers', 'were', 'go', 'school', 'he', 'was', 'a', 'student', 'at', 'naval', 'school', 'but', 'never', 'graduated', 'he', 'went', 'off', 'work', 'on', 'virgin', 'lands', 'one', 'gigantic', 'agricultural', 'projects', 'instituted', 'under', 'khrushchev', 'when', 'he', 'returned', 'he', 'lived', 'baku', 'later', 'moved', 'sumgait', 'helping', 'with', 'town', 's', 'construction', 'mamma', 'was', 'from', 'village', 'dagdagan', 'also', 'from', 'karabagh', 'she', 'worked', 'sumgait', 'first', 'a', 'bookstore', 'later', 'on', 'a', 'construction', 'site', 'my', 'sister', 'is', 'older', 'than', 'i', 'she', 'lives', 'with', 'her', 'husband', 'here', 'karabagh', 'i', 'always', 'loved', 'my', 'parents', 'that', 'was', 'why', 'i', 'went', 'on', 'grade', 'because', 'it', 'was', 'their', 'dream', 'that', 'i', 'would', 'continue', 'my', 'studies', 'i', 'finished', 'grade', 'wanted', 'enter', 'baku', 'nautical', 'school', 'after', 'that', 'military', 'school', 'but', 'later', 'i', 'changed', 'my', 'mind', 'or', 'rather', 'my', 'parents', 'got', 'me', 'recon', 'sider', 'saying', 'that', 'it', 'would', 'be', 'better', 'finish', 'grade', 'then', 'join', 'naval', 'school', 'i', 'was', 'planning', 'be', 'navy', 'almost', 'my', 'whole', 'life', 'long', 'since', 'childhood', 'i', 'had', 'dreamed', 'being', 'a', 'sailor', 'my', 'father', 'wanted', 'it', 'more', 'than', 'anything', 'he', 'always', 'recollected', 'his', 'youth', 'telling', 'school', 'he', 'always', 'said', 'that', 'he', 'had', 'made', 'a', 'big', 'mistake', 'leaving', 'it', 'now', 'i', 'live', 'karabagh', 'never', 'plan', 'leave', 'here', 'i', 'will', 'stay', 'at', 'home', 'my', 'grandfather', 'my', 'ancestors', 'till', 'end', 'my', 'days', 'while', 'hospital', 'baku', 'i', 'learned', 'fates', 'many', 'others', 'who', 'had', 'suffered', 'as', 'well', 'like', 'ishkhan', 'trdatov', 'he', 'managed', 'hold', 'them', 'off', 'at', 'their', 'residence', 'microdistrict', 'building', 'apartment', 'a', 'long', 'time', 'lost', 'his', 'father', 'gabriel', 'by', 'some', 'miracle', 'managed', 'survive', 'i', 'also', 'learned', 'uncle', 'sasha', 'from', 'building', 'whose', 'daughter', 'was', 'raped', 'besides', 'them', 'valery', 'i', 'forgot', 'his', 'last', 'name', 'was', 'hospital', 'too', 'about', 'a', 'year', 'younger', 'than', 'i', 'he', 'went', 'school', 'no', 'he', 'was', 'riding', 'with', 'his', 'parents', 'car', 'people', 'were', 'throwing', 'rocks', 'at', 'them', 'he', 'was', 'hit', 'his', 'parents', 'brought', 'him', 'hospital', 'he', 'was', 'our', 'ward', 'we', 'even', 'came', 'be', 'friends', 'before', 'that', 'we', 'had', 'just', 'seen', 'each', 'other', 'around', 'town', 'but', 'hospital', 'we', 'got', 'know', 'one', 'another', 'better', 'i', 'learned', 'fates', 'others', 'those', 'who', 'had', 'died', 'or', 'who', 'were', 'befallen', 'by', 'misfortune', 'today', 'suren', 'harutunian', 'first', 'secretary', 'communist', 'party', 'armenia', 'was', 'shown', 'on', 'television', 'be', 'honest', 'i', 'am', 'glad', 'that', 'armenia', 'agreed', 'recognize', 'nagorno', 'karabagh', 'as', 'part', 'armenian', 'soviet', 'socialist', 'republic', 'i', 'was', 'repelled', 'no', 'revolted', 'hear', 'baku', 'announcer', 'who', 'read', 'decision', 'azerbaijani', 'supreme', 'soviet', 'presidium', 'against', 'karabagh', 'becoming', 'part', 'armenia', 'after', 'events', 'sumgait', 'those', 'baku', 'best', 'solution', 'is', 'give', 'karabagh', 'armenia', 'return', 'it', 'armenia', 'since', 'people', 'want', 'live', 'peacefully', 'with', 'azerbaijanis', 'but', 'everything', 'has', 'be', 'right', 'before', 'they', 'can', 'do', 'that', 'i', 'arrived', 'karabagh', 'on', 'april', 'i', 'felt', 'very', 'bad', 'i', 'had', 'constant', 'headaches', 'after', 'a', 'while', 'my', 'strength', 'returned', 'my', 'older', 'sister', 'suzanna', 'took', 'me', 'i', 'think', 'that', 'justice', 'should', 'prevail', 'people', 'are', 'demanding', 'their', 'due', 'you', 'can', 't', 'take', 'away', 'what', 'is', 'their', 'due', 'my', 'parents', 'i', 'often', 'spoke', 'nagorno', 'karabagh', 'often', 'visited', 'here', 'spent', 'almost', 'all', 'my', 'vacations', 'here', 'we', 'had', 'even', 'decided', 'that', 'if', 'karabagh', 'would', 'be', 'made', 'part', 'armenia', 'we', 'would', 'move', 'here', 'sure', 'we', 'always', 'said', 'that', 'armenian', 'people', 'had', 'suffered', 'much', 'that', 'what', 'had', 'been', 'done', 'removing', 'nagorno', 'karabagh', 'from', 'armenia', 'was', 'wrong', 'sooner', 'or', 'later', 'mistakes', 'should', 'be', 'corrected', 'order', 'correct', 'a', 'mistake', 'it', 'must', 'not', 'be', 'repeated', 'fate', 'all', 'nagorno', 'karabagh', 'lies', 'hands', 'our', 'government', 'june', 'stepanakert', 'david', 'davidian', 'dbd', 'urartu', 'sdpa', 'org', 'how', 'do', 'we', 'explain', 'turkish', 'troops', 'on', 's', 'd', 'p', 'a', 'center', 'regional', 'studies', 'armenian', 'border', 'when', 'we', 'can', 't', 'p', 'o', 'box', 'even', 'explain', 'cambridge', 'ma', 'turkish', 'mp', 'march'], tags=[0]), TaggedDocument(words=['from', 'rlb534', 'ibm', 'nwscc', 'sea06', 'navy', 'mil', 'subject', 'fastmicro', 'out', 'business', 'organization', 'utexas', 'mail', 'news', 'gateway', 'lines', 'nntp', 'posting', 'host', 'cs', 'utexas', 'edu', 'i', 'heard', 'fastmicro', 'went', 'out', 'business', 'is', 'this', 'true', 'they', 'don', 't', 'answer', 'their', 'number', 'it', 's'], tags=[1])]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "doc2vec_model = True\n",
    "if doc2vec_model == True:\n",
    "    def tag_document(docs):\n",
    "        doc_num = 0\n",
    "        for x in docs:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(x,[doc_num])\n",
    "            doc_num +=1\n",
    "\n",
    "    x_train_doc2vec = list(tag_document(x_train))\n",
    "    x_val_doc2vec = list(tag_document(x_val))\n",
    "    test_x_doc2vec = list(tag_document(new_test_x))\n",
    "    print(x_train_doc2vec[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    生成doc2vec文档模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflesh = False\n",
    "if doc2vec_model == True:\n",
    "    if isfile(models_path + '/doc2vec200.model') and (reflesh == False):\n",
    "        model_doc2vec200 = Doc2Vec.load(models_path + '/doc2vec200.model')\n",
    "    else:\n",
    "        #instantiate the model with 200 dimention\n",
    "        model_doc2vec200 = gensim.models.doc2vec.Doc2Vec(\n",
    "            size=200, min_count=2, iter=55)\n",
    "        #build a vacabulary\n",
    "        model_doc2vec200.build_vocab(x_train_doc2vec)\n",
    "        #Time for train\n",
    "        %time model_doc2vec200.train(x_train_doc2vec, \\\n",
    "                                     total_examples=model_doc2vec200.corpus_count, \\\n",
    "                                     epochs=model_doc2vec200.iter)\n",
    "        model_doc2vec200.save(models_path + \"/doc2vec200.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.27263761e+00 -1.15470171e+00 -2.44008636e+00 -5.39546847e-01\n",
      " -6.62857175e-01 -3.57489079e-01  6.98517919e-01 -3.87005359e-01\n",
      " -3.83485973e-01  1.64236629e+00  1.95837414e+00  2.38276029e+00\n",
      " -2.57053542e+00  7.10057437e-01  1.15207469e+00 -6.10908449e-01\n",
      " -1.86498892e+00 -2.51980484e-01 -4.96989936e-01 -3.23460311e-01\n",
      "  7.87898898e-01 -2.65188241e+00  1.23260999e+00 -1.75981426e+00\n",
      " -8.41965497e-01  2.24783039e+00  6.97231650e-01  1.14927721e+00\n",
      "  5.76548919e-05  1.02474046e+00  1.36734200e+00  6.64757848e-01\n",
      "  1.75399232e+00 -4.91100043e-01 -5.62069297e-01  2.63437700e+00\n",
      " -1.08000767e+00  9.33788240e-01  9.02029350e-02  1.50964427e+00\n",
      "  1.99356568e+00 -7.01731384e-01 -2.20569110e+00  4.06533152e-01\n",
      " -1.71656772e-01  7.34414577e-01  1.25242901e+00  2.90023088e-01\n",
      "  1.64160585e+00 -1.08704722e+00 -1.35905302e+00 -3.35636377e+00\n",
      " -2.71992266e-01  1.74750245e+00  3.03527141e+00  9.73675191e-01\n",
      " -1.89635897e+00  4.33492690e-01 -1.14912462e+00 -1.90651226e+00\n",
      "  1.15490758e+00 -1.99715078e+00 -2.05354214e+00 -3.26652288e-01\n",
      " -7.81696320e-01  2.30343032e+00  9.32765901e-01  8.74855161e-01\n",
      " -1.31542075e+00 -1.98392856e+00 -1.32678464e-01 -7.30686665e-01\n",
      " -1.76518130e+00 -1.07421696e+00  1.13463962e+00  1.99695456e+00\n",
      " -4.78007823e-01 -2.35601276e-01 -8.09080899e-01 -1.67783868e+00\n",
      " -2.03185534e+00 -3.13260168e-01 -1.00974083e+00  8.41594487e-03\n",
      "  8.90061557e-01 -2.11790483e-03  1.85986257e+00 -1.48708236e+00\n",
      " -6.01799250e-01 -1.05647147e-02  2.04975224e+00 -1.82306573e-01\n",
      " -1.35538983e+00 -9.87574384e-02 -7.84332216e-01  1.10568538e-01\n",
      " -1.93744540e+00  1.16526939e-01 -1.88982320e+00  1.67332673e+00\n",
      " -2.26648808e+00  1.70161879e+00  1.40319610e+00  4.19919789e-01\n",
      "  1.22079933e+00 -9.47488368e-01 -1.27440131e+00  2.37640906e-02\n",
      "  6.89206123e-01 -8.73924732e-01 -1.28029153e-01 -3.00880164e-01\n",
      " -2.00558209e+00  8.45681727e-01 -3.85676324e-01 -5.99789619e-01\n",
      "  2.65858245e+00  1.08817124e+00 -5.49702585e-01 -1.63833761e+00\n",
      " -6.43122315e-01 -1.22051477e-01 -7.05261648e-01  3.35697842e+00\n",
      "  1.23676825e+00  8.11887980e-01 -9.87066984e-01  7.78433010e-02\n",
      "  7.70407096e-02  1.99818230e+00 -1.73035920e+00  9.49876666e-01\n",
      "  4.75915015e-01  1.66280836e-01  1.36735237e+00 -6.11973256e-02\n",
      "  1.75438380e+00  8.40699971e-01 -1.33177304e+00 -1.07987273e+00\n",
      "  1.21221161e+00  7.68364012e-01  4.01728183e-01  1.67147803e+00\n",
      "  1.40571868e+00 -6.57508433e-01 -1.32764184e+00  8.00715983e-02\n",
      " -2.73025012e+00  6.78817987e-01  3.72055545e-02  1.94821036e+00\n",
      "  4.69102412e-01 -1.20096914e-01  9.95987713e-01 -4.08168465e-01\n",
      "  5.05703270e-01 -2.19655260e-01  1.14448273e+00  5.16948581e-01\n",
      " -6.45727634e-01  1.26983142e+00 -1.13526249e+00  1.19848585e+00\n",
      "  1.22858691e+00  8.85039866e-01 -1.69307649e+00 -1.30144417e+00\n",
      "  1.62269986e+00 -5.67103446e-01  7.55283296e-01 -3.16750705e-01\n",
      "  1.57050395e+00  3.55219781e-01 -2.09265113e+00  9.03307140e-01\n",
      " -2.47637272e-01  1.36318684e+00 -1.18241620e+00  3.93019570e-03\n",
      "  1.08774805e+00 -7.54435658e-01  1.23889720e+00  7.17462838e-01\n",
      "  2.89533377e-01  1.47810698e+00 -1.30762517e+00 -3.22236031e-01\n",
      "  3.74270034e+00  8.00051630e-01  3.62136476e-02 -2.00104141e+00\n",
      " -4.33459461e-01  5.66528994e-04  9.22681630e-01  5.62404156e-01\n",
      "  1.45327687e+00 -3.43823791e-01  8.20117891e-01  1.71502864e+00]\n"
     ]
    }
   ],
   "source": [
    "if doc2vec_model == True:\n",
    "    iv = model_doc2vec200.infer_vector(x_train_doc2vec[0].words)\n",
    "    print(iv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    生成文档向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doc2vec_model == True:\n",
    "    x_train_doc2vec200 = [model_doc2vec200.infer_vector (x.words) for x in  x_train_doc2vec]\n",
    "    x_val_doc2vec200 = [model_doc2vec200.infer_vector(x.words) for x in x_val_doc2vec]\n",
    "    test_x_doc2vec200 = [model_doc2vec200.infer_vector(x.words) for x in test_x_doc2vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义测评函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def get_f1_score(clf, input_data, target_data):\n",
    "    predict_data = clf.predict(input_data)\n",
    "    f1_macro = f1_score(target_data, predict_data,  average = 'macro')\n",
    "    f1_micro = f1_score(target_data, predict_data,  average = 'micro')\n",
    "    return f1_macro, f1_micro\n",
    "\n",
    "def get_f1_score_pure(input_data, target_data):\n",
    "    f1_macro = f1_score(target_data, input_data,  average = 'macro')\n",
    "    f1_micro = f1_score(target_data, input_data,  average = 'micro')\n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "稀疏矩阵格式的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def sparse_gensim2matrix(corpus):\n",
    "    data = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    line_count = 0\n",
    "    for doc in corpus:  # lsi_corpus_total 是之前由gensim生成的lsi向量\n",
    "        for word in doc:\n",
    "            rows.append(line_count)\n",
    "            cols.append(word[0])\n",
    "            data.append(word[1])\n",
    "        line_count += 1\n",
    "    return csr_matrix((data,(rows,cols)), \n",
    "                      shape = [line_count, len(dictionary)]) # 稀疏向量\n",
    "\n",
    "def gensim2matrix(corpus):\n",
    "    return sparse_gensim2matrix(corpus).toarray()\n",
    "\n",
    "def sparse_gensim2matrix_word2vec(corpus, word2vec_size):\n",
    "    data = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    line_count = 0\n",
    "    model = globals()[\"model_word2vec\" + str(word2vec_size)]\n",
    "    for doc in corpus:  # lsi_corpus_total 是之前由gensim生成的lsi向量\n",
    "        for word in doc:\n",
    "            try:\n",
    "                data.extend(model[getwordfromid(word[0])])\n",
    "            except:\n",
    "                continue\n",
    "            rows.append(line_count)\n",
    "            cols.extend(range(word[0],word[0]+word2vec_size))\n",
    "        line_count += 1\n",
    "    return csr_matrix((data,(rows,cols)), \n",
    "                      shape = [line_count, len(dictionary)*word2vec_size]) # 稀疏向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "linear_svc = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_x_train_sparse = sparse_gensim2matrix(tfidf_x_train)\n",
    "tfidf_x_val_sparse = sparse_gensim2matrix(tfidf_x_val)\n",
    "tfidf_test_x_sparse = sparse_gensim2matrix(tfidf_test_x)\n",
    "\n",
    "linear_svc.fit(tfidf_x_train_sparse, y_train_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF x_train data Macro:0.992947240441655, Micro:0.9930394431554525, x_val data Macro:0.9194971608987638, Micro:0.9164825452938578,test_x data Macro:0.8265814495828332, Micro:0.8305894848645778\n"
     ]
    }
   ],
   "source": [
    "f1_macro_x_train, f1_micro_x_train = get_f1_score( linear_svc, tfidf_x_train_sparse, y_train_le)\n",
    "f1_macro_x_val, f1_micro_x_val = get_f1_score(linear_svc, tfidf_x_val_sparse, y_val_le)\n",
    "f1_macro_test_x, f1_micro_test_x = get_f1_score(linear_svc, tfidf_test_x_sparse, test_y_le)\n",
    "print(\"TFIDF x_train data Macro:{}, Micro:{}, x_val data Macro:{}, Micro:{},test_x data Macro:{}, Micro:{}\".format(f1_macro_x_train, f1_micro_x_train, f1_macro_x_val, f1_micro_x_val, f1_macro_test_x, f1_micro_test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_macro(clf, input_data, target_data):\n",
    "    predict_data = clf.predict(input_data)\n",
    "    f1_macro = f1_score(target_data, predict_data,  average = 'macro')\n",
    "#     f1_micro = f1_score(target_data, predict_data,  average = 'micro')\n",
    "    return f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳参数：{'C': 2}\n",
      "TFIDF x_train data Macro:0.9969957410290817, Micro:0.9970169042094796, x_val data Macro:0.9204082511341147, Micro:0.9173663278833407,test_x data Macro:0.8261424656888107, Micro:0.8301911842804036\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "# TODO：初始化分类器\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# TODO：创建你希望调节的参数列表\n",
    "# parameters = {'C':[1, 5, 10], 'kernel':('linear', 'rbf')}\n",
    "parameters = {'C':[1, 2, 5]}\n",
    "# scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "\n",
    "# TODO：在分类器上使用网格搜索，使用'scorer'作为评价函数\n",
    "grid_obj = GridSearchCV(clf, parameters, get_f1_macro, n_jobs=-1)\n",
    "\n",
    "tfidf_x_train_sparse = sparse_gensim2matrix(tfidf_x_train)\n",
    "tfidf_x_val_sparse = sparse_gensim2matrix(tfidf_x_val)\n",
    "tfidf_test_x_sparse = sparse_gensim2matrix(tfidf_test_x)\n",
    "\n",
    "# TODO：用训练数据拟合网格搜索对象并找到最佳参数\n",
    "grid_obj.fit(tfidf_x_train_sparse,  y_train_le)\n",
    "\n",
    "# 得到estimator\n",
    "best_clf = grid_obj.best_estimator_\n",
    "print(\"最佳参数：%r\" % (grid_obj.best_params_))\n",
    "\n",
    "f1_macro_x_train, f1_micro_x_train = get_f1_score( best_clf, tfidf_x_train_sparse, y_train_le)\n",
    "f1_macro_x_val, f1_micro_x_val = get_f1_score(best_clf, tfidf_x_val_sparse, y_val_le)\n",
    "f1_macro_test_x, f1_micro_test_x = get_f1_score(best_clf, tfidf_test_x_sparse, test_y_le)\n",
    "\n",
    "print(\"TFIDF x_train data Macro:{}, Micro:{}, x_val data Macro:{}, Micro:{},test_x data Macro:{}, Micro:{}\".format(f1_macro_x_train, f1_micro_x_train, f1_macro_x_val, f1_micro_x_val, f1_macro_test_x, f1_micro_test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec200词向量平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word2VEC200 F1 macro on x_train data: 0.5427777511251805 micro 0.5617058888520605; x_val data: macro 0.5291634146433101 micro 0.5475033141847105; test_x data: macro 0.4942809857481009, micro 0.5150026553372278\n"
     ]
    }
   ],
   "source": [
    "linear_svc = svm.SVC(kernel='rbf',  C=2)\n",
    "def svm_on_word2vec(size):\n",
    "    x_train = globals()[\"x_train_average_word_vec\" + str(size)]\n",
    "    y_train = globals()[\"y_train\"]\n",
    "    linear_svc.fit(x_train, y_train)\n",
    "    f1_macro_x_train, f1_micro_x_train = get_f1_score(linear_svc, x_train, y_train)\n",
    "    \n",
    "    x_val = globals()[\"x_val_average_word_vec\" + str(size)]\n",
    "    y_val= globals()[\"y_val\"]\n",
    "    f1_macro_x_val, f1_micro_x_val = get_f1_score(linear_svc, x_val, y_val)\n",
    "    \n",
    "    test_x = globals()[\"test_x_average_word_vec\" + str(size)]\n",
    "    test_y = globals()[\"test_y\"]    \n",
    "    f1_macro_test, f1_micro_test = get_f1_score(linear_svc, test_x, test_y)\n",
    "    \n",
    "    print(\"Average Word2VEC{} F1 macro on x_train data: {} micro {}; x_val data: macro {} micro {}; test_x data: macro {}, micro {}\".format(size, f1_macro_x_train, f1_micro_x_train, f1_macro_x_val, f1_micro_x_val, f1_macro_test, f1_micro_test))    \n",
    "    \n",
    "svm_on_word2vec(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc2vec200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2VEC200 F1 macro on x_train data: 0.7186971009270293 micro 0.7131808639929289; x_val data: macro 0.6000686632688742 micro 0.6058329650905877; test_x data: macro 0.587465244989735, micro 0.5966542750929368\n"
     ]
    }
   ],
   "source": [
    "def svm_on_doc2vec(size):\n",
    "    x_train = globals()[\"x_train_doc2vec\" + str(size)]\n",
    "    y_train = globals()[\"y_train\"]\n",
    "    linear_svc.fit(x_train, y_train)\n",
    "    f1_macro_x_train, f1_micro_x_train = get_f1_score(linear_svc, x_train, y_train)\n",
    "    \n",
    "    x_val = globals()[\"x_val_doc2vec\" + str(size)]\n",
    "    y_val= globals()[\"y_val\"]\n",
    "    f1_macro_x_val, f1_micro_x_val = get_f1_score(linear_svc, x_val, y_val)\n",
    "    \n",
    "    test_x = globals()[\"test_x_doc2vec\" + str(size)]\n",
    "    test_y = globals()[\"test_y\"]    \n",
    "    f1_macro_test, f1_micro_test = get_f1_score(linear_svc, test_x, test_y)\n",
    "    \n",
    "    print(\"Doc2VEC{} F1 macro on x_train data: {} micro {}; x_val data: macro {} micro {}; test_x data: macro {}, micro {}\".format(size, f1_macro_x_train, f1_micro_x_train, f1_macro_x_val, f1_micro_x_val, f1_macro_test, f1_micro_test))    \n",
    "\n",
    "svm_on_doc2vec(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTree\n",
    "\n",
    "    tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DecisionTree = DecisionTreeClassifier(max_depth=90, min_samples_split=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF x_train data Macro:0.8024925619155846, Micro:0.783449342614076,  x_val data Macro:0.6352994059701023, Micro:0.6252761820592134,   test_x data Macro:0.5668220984433341, Micro:0.5520446096654275\n"
     ]
    }
   ],
   "source": [
    "tfidf_x_train_sparse = sparse_gensim2matrix(tfidf_x_train)\n",
    "tfidf_x_val_sparse = sparse_gensim2matrix(tfidf_x_val)\n",
    "tfidf_test_x_sparse = sparse_gensim2matrix(tfidf_test_x)\n",
    "\n",
    "DecisionTree.fit(tfidf_x_train_sparse, y_train_le)\n",
    "\n",
    "f1_macro_x_train, f1_micro_x_train = get_f1_score( DecisionTree, tfidf_x_train_sparse, y_train_le)\n",
    "f1_macro_x_val, f1_micro_x_val = get_f1_score(DecisionTree, tfidf_x_val_sparse, y_val_le)\n",
    "f1_macro_test_x, f1_micro_test_x = get_f1_score(DecisionTree, tfidf_test_x_sparse, test_y_le)\n",
    "print(\"TFIDF x_train data Macro:{}, Micro:{},  x_val data Macro:{}, Micro:{},   test_x data Macro:{}, Micro:{}\".format(f1_macro_x_train, f1_micro_x_train, f1_macro_x_val, f1_micro_x_val, f1_macro_test_x, f1_micro_test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳参数：{'max_depth': 90, 'min_samples_split': 40}\n",
      "TFIDF x_train data Macro:0.7891708376015011, Micro:0.7694177438957022,  x_val data Macro:0.6273071247577596, Micro:0.618647812638091,   test_x data Macro:0.5710434271262865, Micro:0.5557620817843866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TODO：初始化分类器\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# TODO：创建你希望调节的参数列表\n",
    "parameters = {'max_depth':[80, 90, 100], 'min_samples_split':[30, 40]}\n",
    "\n",
    "# TODO：在分类器上使用网格搜索，使用'scorer'作为评价函数\n",
    "grid_obj = GridSearchCV(clf, parameters, get_f1_macro, n_jobs=-1)\n",
    "\n",
    "# TODO：用训练数据拟合网格搜索对象并找到最佳参数\n",
    "grid_obj.fit(tfidf_x_train_sparse,  y_train_le)\n",
    "\n",
    "best_clf = grid_obj.best_estimator_\n",
    "print(\"最佳参数：%r\" % (grid_obj.best_params_))\n",
    "\n",
    "# 得到estimator\n",
    "best_clf = grid_obj.best_estimator_\n",
    "\n",
    "f1_macro_x_train, f1_micro_x_train = get_f1_score( best_clf, tfidf_x_train_sparse, y_train_le)\n",
    "f1_macro_x_val, f1_micro_x_val = get_f1_score(best_clf, tfidf_x_val_sparse, y_val_le)\n",
    "f1_macro_test_x, f1_micro_test_x = get_f1_score(best_clf, tfidf_test_x_sparse, test_y_le)\n",
    "\n",
    "print(\"TFIDF x_train data Macro:{}, Micro:{},  x_val data Macro:{}, Micro:{},   test_x data Macro:{}, Micro:{}\".format(f1_macro_x_train, f1_micro_x_train, f1_macro_x_val, f1_micro_x_val, f1_macro_test_x, f1_micro_test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    word2vec200词向量平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word2VEC200 F1 macro on x_train data: 0.591846108933848 micro 0.5916473317865429; x_val data: macro 0.29985181927253146 micro 0.2969509500662837; test_x data: macro 0.2656551415014298, micro 0.2652681890600106\n"
     ]
    }
   ],
   "source": [
    "def Dtree_on_word2vec(size):\n",
    "    x_train = globals()[\"x_train_average_word_vec\" + str(size)]\n",
    "    y_train = globals()[\"y_train\"]\n",
    "    DecisionTree.fit(x_train, y_train)\n",
    "    f1_macro_x_train, f1_micro_x_train = get_f1_score(DecisionTree, x_train, y_train)\n",
    "    \n",
    "    x_val = globals()[\"x_val_average_word_vec\" + str(size)]\n",
    "    y_val= globals()[\"y_val\"]\n",
    "    f1_macro_x_val, f1_micro_x_val = get_f1_score(DecisionTree, x_val, y_val)\n",
    "    \n",
    "    test_x = globals()[\"test_x_average_word_vec\" + str(size)]\n",
    "    test_y = globals()[\"test_y\"]    \n",
    "    f1_macro_test, f1_micro_test = get_f1_score(DecisionTree, test_x, test_y)\n",
    "    \n",
    "    print(\"Average Word2VEC{} F1 macro on x_train data: {} micro {}; x_val data: macro {} micro {}; test_x data: macro {}, micro {}\".format(size, f1_macro_x_train, f1_micro_x_train, f1_macro_x_val, f1_micro_x_val, f1_macro_test, f1_micro_test))    \n",
    "    \n",
    "Dtree_on_word2vec(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳参数：{'max_depth': 120, 'min_samples_split': 40}\n",
      "Average Word2VEC x_train data Macro:0.5468149705783228, Micro:0.546237984753066,  x_val data Macro:0.29841595778535746, Micro:0.2969509500662837,   test_x data Macro:0.268312086344151, Micro:0.2696494954859267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TODO：初始化分类器\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# TODO：创建你希望调节的参数列表\n",
    "parameters = {'max_depth':[120, 125, 130], 'min_samples_split':[35, 40, 45]}\n",
    "\n",
    "# TODO：在分类器上使用网格搜索，使用'scorer'作为评价函数\n",
    "grid_obj = GridSearchCV(clf, parameters, get_f1_macro, n_jobs=-1)\n",
    "\n",
    "x_train = globals()[\"x_train_average_word_vec\" + str(200)]\n",
    "y_train = globals()[\"y_train\"]\n",
    "\n",
    "x_val = globals()[\"x_val_average_word_vec\" + str(200)]\n",
    "y_val= globals()[\"y_val\"]\n",
    "    \n",
    "test_x = globals()[\"test_x_average_word_vec\" + str(200)]\n",
    "test_y = globals()[\"test_y\"]    \n",
    "\n",
    "# TODO：用训练数据拟合网格搜索对象并找到最佳参数\n",
    "grid_obj.fit(x_train,  y_train)\n",
    "\n",
    "best_clf = grid_obj.best_estimator_\n",
    "print(\"最佳参数：%r\" % (grid_obj.best_params_))\n",
    "\n",
    "# 得到estimator\n",
    "best_clf = grid_obj.best_estimator_\n",
    "\n",
    "f1_macro_x_train, f1_micro_x_train = get_f1_score( best_clf, x_train, y_train)\n",
    "f1_macro_x_val, f1_micro_x_val = get_f1_score(best_clf, x_val, y_val)\n",
    "f1_macro_test_x, f1_micro_test_x = get_f1_score(best_clf, test_x, test_y)\n",
    "\n",
    "print(\"Average Word2VEC x_train data Macro:{}, Micro:{},  x_val data Macro:{}, Micro:{},   test_x data Macro:{}, Micro:{}\".format(f1_macro_x_train, f1_micro_x_train, f1_macro_x_val, f1_micro_x_val, f1_macro_test_x, f1_micro_test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    doc2vec200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2VEC200 F1 macro on x_train data: 0.4741194130854792 micro 0.4768533863661474; x_val data: macro 0.1482283533190964 micro 0.15112682280159082; test_x data: macro 0.16261841821368972, micro 0.16728624535315986\n"
     ]
    }
   ],
   "source": [
    "def Dtree_on_doc2vec(size):\n",
    "    x_train = globals()[\"x_train_doc2vec\" + str(size)]\n",
    "    y_train = globals()[\"y_train\"]\n",
    "    DecisionTree.fit(x_train, y_train)\n",
    "    f1_macro_x_train, f1_micro_x_train = get_f1_score(DecisionTree, x_train, y_train)\n",
    "    \n",
    "    x_val = globals()[\"x_val_doc2vec\" + str(size)]\n",
    "    y_val= globals()[\"y_val\"]\n",
    "    f1_macro_x_val, f1_micro_x_val = get_f1_score(DecisionTree, x_val, y_val)\n",
    "    \n",
    "    test_x = globals()[\"test_x_doc2vec\" + str(size)]\n",
    "    test_y = globals()[\"test_y\"]    \n",
    "    f1_macro_test, f1_micro_test = get_f1_score(DecisionTree, test_x, test_y)\n",
    "    \n",
    "    print(\"Doc2VEC{} F1 macro on x_train data: {} micro {}; x_val data: macro {} micro {}; test_x data: macro {}, micro {}\".format(size, f1_macro_x_train, f1_micro_x_train, f1_macro_x_val, f1_micro_x_val, f1_macro_test, f1_micro_test))    \n",
    "\n",
    "Dtree_on_doc2vec(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳参数：{'max_depth': 125, 'min_samples_split': 45}\n",
      "Average Word2VEC x_train data Macro:0.4125389138279867, Micro:0.4179648657606894,  x_val data Macro:0.15339324536015364, Micro:0.1581970835174547,   test_x data Macro:0.16844888646365214, Micro:0.1737918215613383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TODO：初始化分类器\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# TODO：创建你希望调节的参数列表\n",
    "parameters = {'max_depth':[120, 125, 130], 'min_samples_split':[40, 45, 50]}\n",
    "\n",
    "# TODO：在分类器上使用网格搜索，使用'scorer'作为评价函数\n",
    "grid_obj = GridSearchCV(clf, parameters, get_f1_macro, n_jobs=-1)\n",
    "\n",
    "x_train = globals()[\"x_train_doc2vec\" + str(200)]\n",
    "y_train = globals()[\"y_train\"]\n",
    "\n",
    "x_val = globals()[\"x_val_doc2vec\" + str(200)]\n",
    "y_val= globals()[\"y_val\"]\n",
    "    \n",
    "test_x = globals()[\"test_x_doc2vec\" + str(200)]\n",
    "test_y = globals()[\"test_y\"]    \n",
    "\n",
    "# TODO：用训练数据拟合网格搜索对象并找到最佳参数\n",
    "grid_obj.fit(x_train,  y_train)\n",
    "\n",
    "best_clf = grid_obj.best_estimator_\n",
    "print(\"最佳参数：%r\" % (grid_obj.best_params_))\n",
    "\n",
    "# 得到estimator\n",
    "best_clf = grid_obj.best_estimator_\n",
    "\n",
    "f1_macro_x_train, f1_micro_x_train = get_f1_score( best_clf, x_train, y_train)\n",
    "f1_macro_x_val, f1_micro_x_val = get_f1_score(best_clf, x_val, y_val)\n",
    "f1_macro_test_x, f1_micro_test_x = get_f1_score(best_clf, test_x, test_y)\n",
    "\n",
    "print(\"Average Word2VEC x_train data Macro:{}, Micro:{},  x_val data Macro:{}, Micro:{},   test_x data Macro:{}, Micro:{}\".format(f1_macro_x_train, f1_micro_x_train, f1_macro_x_val, f1_micro_x_val, f1_macro_test_x, f1_micro_test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_feature_input(feature_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of feature input\n",
    "    : feature_shape: Shape of the features\n",
    "    : return: Tensor for feature input.\n",
    "    \"\"\"\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape= ([None] + list(feature_shape)), name='x')\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    \n",
    "    y = tf.placeholder(tf.float32, shape=[None, n_classes],name='y')\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    return keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_conn(x_tensor, conv_num_outputs, conv_ksize, conv_strides):\n",
    "    \"\"\"\n",
    "    Apply 1d convolution to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 1-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 1-D Tuple for convolution\n",
    "    : return: A tensor that represents convolution of x_tensor\n",
    "    \"\"\"\n",
    "    conv_depth = x_tensor.get_shape()[1].value\n",
    "    F_W = tf.Variable(tf.truncated_normal(list(conv_ksize) + conv_depth + conv_num_outputs, stddev=0.05))\n",
    "    F_b = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    padding = 'SAME'\n",
    "    x_tensor = tf.nn.conv1d(x_tensor, F_W, [1 + list(conv_strides) + 1], padding) \n",
    "    x_tensor = tf.nn.bias_add(x_tensor, F_b)\n",
    "    x_tensor = tf.nn.relu(x_tensor)\n",
    "    return x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply 1d convolution to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 1-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 1-D Tuple for convolution\n",
    "    : return: A tensor that represents convolution of x_tensor\n",
    "    \"\"\"\n",
    "    conv_depth = x_tensor.get_shape()[1].value\n",
    "    F_W = tf.Variable(tf.truncated_normal(list(conv_ksize) + conv_depth + conv_num_outputs, stddev=0.05))\n",
    "    F_b = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    padding = 'SAME'\n",
    "    x_tensor = tf.nn.conv1d(x_tensor, F_W, [1 + list(conv_strides) + 1], padding) \n",
    "    x_tensor = tf.nn.bias_add(x_tensor, F_b)\n",
    "    x_tensor = tf.nn.relu(x_tensor)\n",
    "    x_tensor = tf.nn.max_pool(x_tensor, [1 + list(pool_ksize) + 1], [1 + list(pool_strides) + 1], padding) \n",
    "    return x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    num_inputs = x_tensor.get_shape()[1].value\n",
    "    f_weight = tf.Variable(tf.truncated_normal([num_inputs, num_outputs], stddev=0.05))\n",
    "    f_bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    fc1 = tf.add(tf.matmul(x_tensor, f_weight),  f_bias)\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    return fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    num_inputs = x_tensor.get_shape()[1].value\n",
    "    f_weight = tf.Variable(tf.truncated_normal([num_inputs, num_outputs],  stddev=0.05))\n",
    "    f_bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    out = tf.add(tf.matmul(x_tensor, f_weight), f_bias)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputsb\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc1 = fully_conn(conv1, 512)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    fc2 = fully_conn(fc1, 1024)\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(fc2, 20)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_net(x_size, y_size):\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "    x = neural_net_feature_input(x_size)\n",
    "    y = neural_net_label_input(y_size)\n",
    "    keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "    # Model\n",
    "\n",
    "    fc1 = fully_conn(x, y_size * 50)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    fc2 = fully_conn(fc1, y_size * 50)\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    out = output(fc2,y_size )\n",
    "    # Name logits Tensor, so that is can be loaded from disk after training\n",
    "    logits = tf.identity(out, name='logits')\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "    \n",
    "    y_pred = tf.argmax(logits, 1, name = \"pred\")\n",
    "\n",
    "    arg_prediction = tf.argmax(logits, 1)\n",
    "    arg_y = tf.argmax(y, 1)\n",
    "    TP = tf.count_nonzero(arg_prediction * arg_y, dtype=tf.float32)\n",
    "    TN = tf.count_nonzero((arg_prediction - 1) * (arg_y - 1), dtype=tf.float32)\n",
    "    \n",
    "    FN = tf.count_nonzero( (arg_prediction - 1) * arg_y, dtype=tf.float32)\n",
    "    FP = tf.count_nonzero( arg_prediction * (arg_y - 1), dtype=tf.float32)\n",
    "    \n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "   \n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    f1_macro = tf.reduce_mean(tf.cast(f1, tf.float32), name = 'tf_macro')\n",
    "    \n",
    "    return tf, x, y, keep_prob, optimizer, cost, accuracy, f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, x, y, \n",
    "                keep_prob, cost, accuracy):\n",
    "    loss = session.run(cost, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.})\n",
    "    valid_acc = session.run(accuracy, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.})\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                loss,\n",
    "                valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.sys.mac.hardware', 'comp.sys.ibm.pc.hardware', 'comp.os.ms-windows.misc', 'comp.sys.mac.hardware', 'alt.atheism', 'comp.windows.x', 'sci.electronics', 'comp.windows.x', 'alt.atheism', 'comp.os.ms-windows.misc']\n",
      "['talk.politics.misc', 'sci.electronics', 'talk.politics.misc', 'comp.sys.ibm.pc.hardware', 'comp.graphics', 'talk.politics.misc', 'rec.autos', 'misc.forsale', 'comp.sys.mac.hardware', 'talk.politics.mideast']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#准备批输入\n",
    "import random\n",
    "def load_preprocess_training_batch(corpus, label, batch_size):\n",
    "    c = list(zip(corpus, label))\n",
    "    random.shuffle(c)\n",
    "    temp = list(zip(*c))\n",
    "    corpus = list(temp[0])\n",
    "    label = list(temp[1])\n",
    "    for start in range(0, len(corpus), batch_size):\n",
    "        #ignore the last batch in case it's less than batch_size\n",
    "        if start + batch_size <= len(corpus):\n",
    "            batch_end = start + batch_size\n",
    "            yield corpus[start:batch_end], label[start:batch_end]\n",
    "#测试            \n",
    "[print(e) for i, e in load_preprocess_training_batch(x_train[:20], y_train[:20], 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_full_net(corpus, labels, x_size, y_size, model_path):\n",
    "    tf, x, y, keep_prob, optimizer, cost, accuracy, f1_macro = full_net(x_size,y_size)\n",
    "    save_model_full_net_path = model_path\n",
    "    epochs = 20\n",
    "    keep_probability = 0.75 \n",
    "    batch_size = 36\n",
    "    print('Checking the Training on a Single Batch...')\n",
    "    print(tf)\n",
    "    with tf.Session() as sess:\n",
    "        # Initializing the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(epochs):\n",
    "            batch_i = 0\n",
    "            print(\"Epoch{} start:\".format(epoch))\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(\n",
    "                corpus, labels, batch_size):\n",
    "                sess.run(optimizer, \n",
    "                         feed_dict = {x: batch_features , y: batch_labels, \n",
    "                                      keep_prob: keep_probability})\n",
    "                if batch_i % 20 == 0:\n",
    "                    print('Epoch {:>2},  Batch {}:  '.format(\n",
    "                        epoch, batch_i), end='')\n",
    "                    print_stats(sess, batch_features, batch_labels, x, y, \n",
    "                keep_prob,cost, accuracy)\n",
    "                batch_i +=1\n",
    "        #保存模型\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, save_model_full_net_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "#模型验证            \n",
    "def test_model(model_path, x_train, y_train, \n",
    "               x_val, y_val, test_x, test_y):\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "    batch_size = 128\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(model_path + '.meta')\n",
    "        loader.restore(sess, model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        loaded_pred = loaded_graph.get_tensor_by_name('pred:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        y_output = []\n",
    "        y_target = []\n",
    "        for train_feature_batch, train_label_batch in load_preprocess_training_batch(x_train, y_train, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, \n",
    "                           loaded_y: train_label_batch, \n",
    "                           loaded_keep_prob: 1.0})\n",
    "            \n",
    "            y_output += sess.run(\n",
    "                    loaded_pred,\n",
    "                    feed_dict={loaded_x: train_feature_batch,\n",
    "                               loaded_keep_prob: 1.0}).tolist()\n",
    "            \n",
    "            y_target += train_label_batch\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print(\"batch output len:{} output:{}\".format(\n",
    "            len(y_output),y_output[:100]))\n",
    "        print('x_train Testing Accuracy: {}\\n'.format(\n",
    "                        test_batch_acc_total/test_batch_count))\n",
    "        y_target = [np.argmax(x) for x in y_target]\n",
    "\n",
    "        f1_macro, f1_micro = get_f1_score_pure(\n",
    "            y_target[:len(y_output)], y_output)\n",
    "        print('x_train Testing F1 Macro {}    F1 Micro {}'.format(f1_macro, f1_micro))\n",
    "\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        y_output = []\n",
    "        y_target = []\n",
    "        for train_feature_batch, train_label_batch in load_preprocess_training_batch(x_val, y_val, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, \n",
    "                           loaded_y: train_label_batch, \n",
    "                           loaded_keep_prob: 1.0})\n",
    "            \n",
    "            y_output += sess.run(\n",
    "                loaded_pred,\n",
    "                feed_dict={loaded_x: train_feature_batch, \n",
    "                           #loaded_y: train_label_batch, \n",
    "                           loaded_keep_prob: 1.0}).tolist()\n",
    "            \n",
    "            y_target += train_label_batch\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('x_val Testing Accuracy: {}\\n'.format(\n",
    "            test_batch_acc_total/test_batch_count))\n",
    "        y_target = [np.argmax(x) for x in y_target]\n",
    "        f1_macro, f1_micro = get_f1_score_pure(\n",
    "            y_target[:len(y_output)], y_output)\n",
    "        print('x_val Testing F1 Macro {}    F1 Micro {}'.format(f1_macro, f1_micro))\n",
    "\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        y_output = []\n",
    "        y_target = []\n",
    "        for train_feature_batch, train_label_batch in load_preprocess_training_batch(test_x, test_y, batch_size):\n",
    "\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, \n",
    "                           loaded_y: train_label_batch, \n",
    "                           loaded_keep_prob: 1.0})\n",
    "            \n",
    "            y_output += sess.run(\n",
    "                    loaded_pred,\n",
    "                    feed_dict={loaded_x: train_feature_batch, \n",
    "                               #loaded_y: train_label_batch,\n",
    "                               loaded_keep_prob: 1.0}).tolist()\n",
    "            \n",
    "            y_target += train_label_batch\n",
    "            test_batch_count += 1\n",
    "            \n",
    "        print('Test _X Testing Accuracy: {}\\n'.format(\n",
    "            test_batch_acc_total/test_batch_count))\n",
    "        print(\"test:{}\".format(y_output[:10]))\n",
    "        #test_y = [np.argmax(x) for x in test_y]\n",
    "        y_target = [np.argmax(x) for x in y_target]\n",
    "        f1_macro, f1_micro = get_f1_score_pure(\n",
    "            y_target[:len(y_output)], y_output)\n",
    "        print('Test _X Testing F1 Macro {}    F1 Micro {}'.format(f1_macro, f1_micro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec200词向量平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-55-178da4620d1c>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Checking the Training on a Single Batch...\n",
      "<module 'tensorflow' from '/home/chronosvv/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/__init__.py'>\n",
      "Epoch0 start:\n",
      "Epoch  0,  Batch 0:  Loss:     2.8167 Validation Accuracy: 0.166667\n",
      "Epoch  0,  Batch 20:  Loss:     2.7955 Validation Accuracy: 0.166667\n",
      "Epoch  0,  Batch 40:  Loss:     2.4896 Validation Accuracy: 0.277778\n",
      "Epoch  0,  Batch 60:  Loss:     2.0984 Validation Accuracy: 0.361111\n",
      "Epoch  0,  Batch 80:  Loss:     2.0346 Validation Accuracy: 0.444444\n",
      "Epoch  0,  Batch 100:  Loss:     2.1312 Validation Accuracy: 0.361111\n",
      "Epoch  0,  Batch 120:  Loss:     2.1923 Validation Accuracy: 0.416667\n",
      "Epoch  0,  Batch 140:  Loss:     2.0776 Validation Accuracy: 0.361111\n",
      "Epoch  0,  Batch 160:  Loss:     1.9663 Validation Accuracy: 0.333333\n",
      "Epoch  0,  Batch 180:  Loss:     1.7790 Validation Accuracy: 0.388889\n",
      "Epoch  0,  Batch 200:  Loss:     2.0205 Validation Accuracy: 0.277778\n",
      "Epoch  0,  Batch 220:  Loss:     1.9978 Validation Accuracy: 0.250000\n",
      "Epoch  0,  Batch 240:  Loss:     2.0743 Validation Accuracy: 0.333333\n",
      "Epoch1 start:\n",
      "Epoch  1,  Batch 0:  Loss:     1.9091 Validation Accuracy: 0.361111\n",
      "Epoch  1,  Batch 20:  Loss:     1.8216 Validation Accuracy: 0.444444\n",
      "Epoch  1,  Batch 40:  Loss:     1.6909 Validation Accuracy: 0.416667\n",
      "Epoch  1,  Batch 60:  Loss:     1.6168 Validation Accuracy: 0.527778\n",
      "Epoch  1,  Batch 80:  Loss:     1.5314 Validation Accuracy: 0.472222\n",
      "Epoch  1,  Batch 100:  Loss:     1.8186 Validation Accuracy: 0.305556\n",
      "Epoch  1,  Batch 120:  Loss:     1.6529 Validation Accuracy: 0.305556\n",
      "Epoch  1,  Batch 140:  Loss:     1.4668 Validation Accuracy: 0.500000\n",
      "Epoch  1,  Batch 160:  Loss:     1.8800 Validation Accuracy: 0.472222\n",
      "Epoch  1,  Batch 180:  Loss:     1.6474 Validation Accuracy: 0.277778\n",
      "Epoch  1,  Batch 200:  Loss:     1.9304 Validation Accuracy: 0.305556\n",
      "Epoch  1,  Batch 220:  Loss:     1.8347 Validation Accuracy: 0.416667\n",
      "Epoch  1,  Batch 240:  Loss:     1.7523 Validation Accuracy: 0.333333\n",
      "Epoch2 start:\n",
      "Epoch  2,  Batch 0:  Loss:     1.8806 Validation Accuracy: 0.333333\n",
      "Epoch  2,  Batch 20:  Loss:     1.4540 Validation Accuracy: 0.638889\n",
      "Epoch  2,  Batch 40:  Loss:     1.5101 Validation Accuracy: 0.472222\n",
      "Epoch  2,  Batch 60:  Loss:     1.4573 Validation Accuracy: 0.583333\n",
      "Epoch  2,  Batch 80:  Loss:     1.5400 Validation Accuracy: 0.444444\n",
      "Epoch  2,  Batch 100:  Loss:     1.4111 Validation Accuracy: 0.472222\n",
      "Epoch  2,  Batch 120:  Loss:     1.8607 Validation Accuracy: 0.333333\n",
      "Epoch  2,  Batch 140:  Loss:     1.8025 Validation Accuracy: 0.444444\n",
      "Epoch  2,  Batch 160:  Loss:     1.7933 Validation Accuracy: 0.388889\n",
      "Epoch  2,  Batch 180:  Loss:     1.6246 Validation Accuracy: 0.500000\n",
      "Epoch  2,  Batch 200:  Loss:     1.7019 Validation Accuracy: 0.472222\n",
      "Epoch  2,  Batch 220:  Loss:     1.6497 Validation Accuracy: 0.472222\n",
      "Epoch  2,  Batch 240:  Loss:     1.1928 Validation Accuracy: 0.555556\n",
      "Epoch3 start:\n",
      "Epoch  3,  Batch 0:  Loss:     1.5801 Validation Accuracy: 0.500000\n",
      "Epoch  3,  Batch 20:  Loss:     1.9583 Validation Accuracy: 0.361111\n",
      "Epoch  3,  Batch 40:  Loss:     1.3015 Validation Accuracy: 0.472222\n",
      "Epoch  3,  Batch 60:  Loss:     1.6944 Validation Accuracy: 0.472222\n",
      "Epoch  3,  Batch 80:  Loss:     1.5475 Validation Accuracy: 0.416667\n",
      "Epoch  3,  Batch 100:  Loss:     1.4088 Validation Accuracy: 0.666667\n",
      "Epoch  3,  Batch 120:  Loss:     1.7446 Validation Accuracy: 0.555556\n",
      "Epoch  3,  Batch 140:  Loss:     1.5953 Validation Accuracy: 0.416667\n",
      "Epoch  3,  Batch 160:  Loss:     1.4853 Validation Accuracy: 0.444444\n",
      "Epoch  3,  Batch 180:  Loss:     1.3660 Validation Accuracy: 0.611111\n",
      "Epoch  3,  Batch 200:  Loss:     1.6330 Validation Accuracy: 0.388889\n",
      "Epoch  3,  Batch 220:  Loss:     1.7330 Validation Accuracy: 0.416667\n",
      "Epoch  3,  Batch 240:  Loss:     1.7113 Validation Accuracy: 0.416667\n",
      "Epoch4 start:\n",
      "Epoch  4,  Batch 0:  Loss:     1.8960 Validation Accuracy: 0.305556\n",
      "Epoch  4,  Batch 20:  Loss:     1.4964 Validation Accuracy: 0.472222\n",
      "Epoch  4,  Batch 40:  Loss:     1.3889 Validation Accuracy: 0.583333\n",
      "Epoch  4,  Batch 60:  Loss:     1.5839 Validation Accuracy: 0.361111\n",
      "Epoch  4,  Batch 80:  Loss:     1.6466 Validation Accuracy: 0.527778\n",
      "Epoch  4,  Batch 100:  Loss:     1.6586 Validation Accuracy: 0.555556\n",
      "Epoch  4,  Batch 120:  Loss:     1.2691 Validation Accuracy: 0.611111\n",
      "Epoch  4,  Batch 140:  Loss:     1.6865 Validation Accuracy: 0.416667\n",
      "Epoch  4,  Batch 160:  Loss:     1.3334 Validation Accuracy: 0.555556\n",
      "Epoch  4,  Batch 180:  Loss:     1.1841 Validation Accuracy: 0.611111\n",
      "Epoch  4,  Batch 200:  Loss:     1.4669 Validation Accuracy: 0.583333\n",
      "Epoch  4,  Batch 220:  Loss:     1.4454 Validation Accuracy: 0.583333\n",
      "Epoch  4,  Batch 240:  Loss:     1.3153 Validation Accuracy: 0.583333\n",
      "Epoch5 start:\n",
      "Epoch  5,  Batch 0:  Loss:     1.2623 Validation Accuracy: 0.666667\n",
      "Epoch  5,  Batch 20:  Loss:     1.5848 Validation Accuracy: 0.527778\n",
      "Epoch  5,  Batch 40:  Loss:     1.6297 Validation Accuracy: 0.444444\n",
      "Epoch  5,  Batch 60:  Loss:     1.5010 Validation Accuracy: 0.444444\n",
      "Epoch  5,  Batch 80:  Loss:     1.4970 Validation Accuracy: 0.416667\n",
      "Epoch  5,  Batch 100:  Loss:     1.6350 Validation Accuracy: 0.472222\n",
      "Epoch  5,  Batch 120:  Loss:     1.4538 Validation Accuracy: 0.500000\n",
      "Epoch  5,  Batch 140:  Loss:     1.3386 Validation Accuracy: 0.555556\n",
      "Epoch  5,  Batch 160:  Loss:     1.3989 Validation Accuracy: 0.555556\n",
      "Epoch  5,  Batch 180:  Loss:     1.7978 Validation Accuracy: 0.444444\n",
      "Epoch  5,  Batch 200:  Loss:     1.2736 Validation Accuracy: 0.527778\n",
      "Epoch  5,  Batch 220:  Loss:     1.4580 Validation Accuracy: 0.472222\n",
      "Epoch  5,  Batch 240:  Loss:     1.5348 Validation Accuracy: 0.388889\n",
      "Epoch6 start:\n",
      "Epoch  6,  Batch 0:  Loss:     1.5179 Validation Accuracy: 0.416667\n",
      "Epoch  6,  Batch 20:  Loss:     1.6249 Validation Accuracy: 0.472222\n",
      "Epoch  6,  Batch 40:  Loss:     1.6083 Validation Accuracy: 0.527778\n",
      "Epoch  6,  Batch 60:  Loss:     1.7291 Validation Accuracy: 0.444444\n",
      "Epoch  6,  Batch 80:  Loss:     1.3948 Validation Accuracy: 0.611111\n",
      "Epoch  6,  Batch 100:  Loss:     1.5320 Validation Accuracy: 0.444444\n",
      "Epoch  6,  Batch 120:  Loss:     1.4954 Validation Accuracy: 0.500000\n",
      "Epoch  6,  Batch 140:  Loss:     1.6287 Validation Accuracy: 0.527778\n",
      "Epoch  6,  Batch 160:  Loss:     1.5427 Validation Accuracy: 0.416667\n",
      "Epoch  6,  Batch 180:  Loss:     1.6098 Validation Accuracy: 0.444444\n",
      "Epoch  6,  Batch 200:  Loss:     1.2020 Validation Accuracy: 0.583333\n",
      "Epoch  6,  Batch 220:  Loss:     1.3904 Validation Accuracy: 0.500000\n",
      "Epoch  6,  Batch 240:  Loss:     1.5935 Validation Accuracy: 0.444444\n",
      "Epoch7 start:\n",
      "Epoch  7,  Batch 0:  Loss:     1.7176 Validation Accuracy: 0.500000\n",
      "Epoch  7,  Batch 20:  Loss:     1.5224 Validation Accuracy: 0.527778\n",
      "Epoch  7,  Batch 40:  Loss:     1.2239 Validation Accuracy: 0.583333\n",
      "Epoch  7,  Batch 60:  Loss:     1.2437 Validation Accuracy: 0.638889\n",
      "Epoch  7,  Batch 80:  Loss:     1.5524 Validation Accuracy: 0.527778\n",
      "Epoch  7,  Batch 100:  Loss:     1.2084 Validation Accuracy: 0.611111\n",
      "Epoch  7,  Batch 120:  Loss:     2.2159 Validation Accuracy: 0.361111\n",
      "Epoch  7,  Batch 140:  Loss:     1.1001 Validation Accuracy: 0.611111\n",
      "Epoch  7,  Batch 160:  Loss:     1.5398 Validation Accuracy: 0.444444\n",
      "Epoch  7,  Batch 180:  Loss:     1.1941 Validation Accuracy: 0.611111\n",
      "Epoch  7,  Batch 200:  Loss:     1.3847 Validation Accuracy: 0.527778\n",
      "Epoch  7,  Batch 220:  Loss:     1.5796 Validation Accuracy: 0.444444\n",
      "Epoch  7,  Batch 240:  Loss:     1.4789 Validation Accuracy: 0.444444\n",
      "Epoch8 start:\n",
      "Epoch  8,  Batch 0:  Loss:     1.2677 Validation Accuracy: 0.583333\n",
      "Epoch  8,  Batch 20:  Loss:     1.4167 Validation Accuracy: 0.527778\n",
      "Epoch  8,  Batch 40:  Loss:     1.5811 Validation Accuracy: 0.472222\n",
      "Epoch  8,  Batch 60:  Loss:     1.3377 Validation Accuracy: 0.555556\n",
      "Epoch  8,  Batch 80:  Loss:     1.5254 Validation Accuracy: 0.527778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8,  Batch 100:  Loss:     1.1316 Validation Accuracy: 0.500000\n",
      "Epoch  8,  Batch 120:  Loss:     1.5224 Validation Accuracy: 0.416667\n",
      "Epoch  8,  Batch 140:  Loss:     1.2631 Validation Accuracy: 0.500000\n",
      "Epoch  8,  Batch 160:  Loss:     1.8336 Validation Accuracy: 0.472222\n",
      "Epoch  8,  Batch 180:  Loss:     1.4195 Validation Accuracy: 0.527778\n",
      "Epoch  8,  Batch 200:  Loss:     1.4376 Validation Accuracy: 0.444444\n",
      "Epoch  8,  Batch 220:  Loss:     1.6247 Validation Accuracy: 0.361111\n",
      "Epoch  8,  Batch 240:  Loss:     1.3188 Validation Accuracy: 0.583333\n",
      "Epoch9 start:\n",
      "Epoch  9,  Batch 0:  Loss:     1.3258 Validation Accuracy: 0.555556\n",
      "Epoch  9,  Batch 20:  Loss:     1.0825 Validation Accuracy: 0.611111\n",
      "Epoch  9,  Batch 40:  Loss:     1.3362 Validation Accuracy: 0.555556\n",
      "Epoch  9,  Batch 60:  Loss:     1.3931 Validation Accuracy: 0.472222\n",
      "Epoch  9,  Batch 80:  Loss:     1.2871 Validation Accuracy: 0.611111\n",
      "Epoch  9,  Batch 100:  Loss:     1.2615 Validation Accuracy: 0.583333\n",
      "Epoch  9,  Batch 120:  Loss:     1.7201 Validation Accuracy: 0.416667\n",
      "Epoch  9,  Batch 140:  Loss:     1.9475 Validation Accuracy: 0.333333\n",
      "Epoch  9,  Batch 160:  Loss:     1.3853 Validation Accuracy: 0.611111\n",
      "Epoch  9,  Batch 180:  Loss:     1.2659 Validation Accuracy: 0.611111\n",
      "Epoch  9,  Batch 200:  Loss:     1.8367 Validation Accuracy: 0.444444\n",
      "Epoch  9,  Batch 220:  Loss:     1.5184 Validation Accuracy: 0.472222\n",
      "Epoch  9,  Batch 240:  Loss:     1.2767 Validation Accuracy: 0.611111\n",
      "Epoch10 start:\n",
      "Epoch 10,  Batch 0:  Loss:     1.4486 Validation Accuracy: 0.472222\n",
      "Epoch 10,  Batch 20:  Loss:     1.2963 Validation Accuracy: 0.694444\n",
      "Epoch 10,  Batch 40:  Loss:     1.8259 Validation Accuracy: 0.305556\n",
      "Epoch 10,  Batch 60:  Loss:     1.3102 Validation Accuracy: 0.555556\n",
      "Epoch 10,  Batch 80:  Loss:     1.3451 Validation Accuracy: 0.638889\n",
      "Epoch 10,  Batch 100:  Loss:     1.5316 Validation Accuracy: 0.416667\n",
      "Epoch 10,  Batch 120:  Loss:     1.3481 Validation Accuracy: 0.527778\n",
      "Epoch 10,  Batch 140:  Loss:     1.4238 Validation Accuracy: 0.583333\n",
      "Epoch 10,  Batch 160:  Loss:     1.0886 Validation Accuracy: 0.666667\n",
      "Epoch 10,  Batch 180:  Loss:     1.6587 Validation Accuracy: 0.527778\n",
      "Epoch 10,  Batch 200:  Loss:     1.4580 Validation Accuracy: 0.555556\n",
      "Epoch 10,  Batch 220:  Loss:     1.2355 Validation Accuracy: 0.638889\n",
      "Epoch 10,  Batch 240:  Loss:     1.5208 Validation Accuracy: 0.444444\n",
      "Epoch11 start:\n",
      "Epoch 11,  Batch 0:  Loss:     1.5366 Validation Accuracy: 0.555556\n",
      "Epoch 11,  Batch 20:  Loss:     1.6000 Validation Accuracy: 0.472222\n",
      "Epoch 11,  Batch 40:  Loss:     1.2611 Validation Accuracy: 0.555556\n",
      "Epoch 11,  Batch 60:  Loss:     1.2700 Validation Accuracy: 0.527778\n",
      "Epoch 11,  Batch 80:  Loss:     1.2790 Validation Accuracy: 0.638889\n",
      "Epoch 11,  Batch 100:  Loss:     1.5073 Validation Accuracy: 0.500000\n",
      "Epoch 11,  Batch 120:  Loss:     1.4553 Validation Accuracy: 0.555556\n",
      "Epoch 11,  Batch 140:  Loss:     1.8426 Validation Accuracy: 0.305556\n",
      "Epoch 11,  Batch 160:  Loss:     1.4040 Validation Accuracy: 0.611111\n",
      "Epoch 11,  Batch 180:  Loss:     1.1653 Validation Accuracy: 0.638889\n",
      "Epoch 11,  Batch 200:  Loss:     1.6202 Validation Accuracy: 0.444444\n",
      "Epoch 11,  Batch 220:  Loss:     1.3133 Validation Accuracy: 0.611111\n",
      "Epoch 11,  Batch 240:  Loss:     1.4322 Validation Accuracy: 0.583333\n",
      "Epoch12 start:\n",
      "Epoch 12,  Batch 0:  Loss:     1.2783 Validation Accuracy: 0.527778\n",
      "Epoch 12,  Batch 20:  Loss:     1.4438 Validation Accuracy: 0.416667\n",
      "Epoch 12,  Batch 40:  Loss:     1.4025 Validation Accuracy: 0.583333\n",
      "Epoch 12,  Batch 60:  Loss:     1.3178 Validation Accuracy: 0.555556\n",
      "Epoch 12,  Batch 80:  Loss:     1.1030 Validation Accuracy: 0.694444\n",
      "Epoch 12,  Batch 100:  Loss:     1.5241 Validation Accuracy: 0.416667\n",
      "Epoch 12,  Batch 120:  Loss:     1.2404 Validation Accuracy: 0.611111\n",
      "Epoch 12,  Batch 140:  Loss:     1.3236 Validation Accuracy: 0.500000\n",
      "Epoch 12,  Batch 160:  Loss:     1.3522 Validation Accuracy: 0.472222\n",
      "Epoch 12,  Batch 180:  Loss:     1.0038 Validation Accuracy: 0.666667\n",
      "Epoch 12,  Batch 200:  Loss:     1.2488 Validation Accuracy: 0.527778\n",
      "Epoch 12,  Batch 220:  Loss:     1.1787 Validation Accuracy: 0.583333\n",
      "Epoch 12,  Batch 240:  Loss:     1.5178 Validation Accuracy: 0.472222\n",
      "Epoch13 start:\n",
      "Epoch 13,  Batch 0:  Loss:     1.3948 Validation Accuracy: 0.666667\n",
      "Epoch 13,  Batch 20:  Loss:     0.9914 Validation Accuracy: 0.722222\n",
      "Epoch 13,  Batch 40:  Loss:     1.3694 Validation Accuracy: 0.694444\n",
      "Epoch 13,  Batch 60:  Loss:     1.1821 Validation Accuracy: 0.611111\n",
      "Epoch 13,  Batch 80:  Loss:     1.2390 Validation Accuracy: 0.555556\n",
      "Epoch 13,  Batch 100:  Loss:     1.1172 Validation Accuracy: 0.555556\n",
      "Epoch 13,  Batch 120:  Loss:     1.4013 Validation Accuracy: 0.527778\n",
      "Epoch 13,  Batch 140:  Loss:     0.9946 Validation Accuracy: 0.694444\n",
      "Epoch 13,  Batch 160:  Loss:     1.3678 Validation Accuracy: 0.500000\n",
      "Epoch 13,  Batch 180:  Loss:     1.4481 Validation Accuracy: 0.527778\n",
      "Epoch 13,  Batch 200:  Loss:     1.7508 Validation Accuracy: 0.444444\n",
      "Epoch 13,  Batch 220:  Loss:     1.3584 Validation Accuracy: 0.555556\n",
      "Epoch 13,  Batch 240:  Loss:     1.3748 Validation Accuracy: 0.583333\n",
      "Epoch14 start:\n",
      "Epoch 14,  Batch 0:  Loss:     1.1050 Validation Accuracy: 0.611111\n",
      "Epoch 14,  Batch 20:  Loss:     1.5325 Validation Accuracy: 0.611111\n",
      "Epoch 14,  Batch 40:  Loss:     1.2562 Validation Accuracy: 0.555556\n",
      "Epoch 14,  Batch 60:  Loss:     1.3795 Validation Accuracy: 0.555556\n",
      "Epoch 14,  Batch 80:  Loss:     1.3769 Validation Accuracy: 0.555556\n",
      "Epoch 14,  Batch 100:  Loss:     1.1805 Validation Accuracy: 0.555556\n",
      "Epoch 14,  Batch 120:  Loss:     0.9306 Validation Accuracy: 0.611111\n",
      "Epoch 14,  Batch 140:  Loss:     1.3988 Validation Accuracy: 0.527778\n",
      "Epoch 14,  Batch 160:  Loss:     1.1775 Validation Accuracy: 0.583333\n",
      "Epoch 14,  Batch 180:  Loss:     1.4816 Validation Accuracy: 0.444444\n",
      "Epoch 14,  Batch 200:  Loss:     1.4527 Validation Accuracy: 0.472222\n",
      "Epoch 14,  Batch 220:  Loss:     1.4282 Validation Accuracy: 0.444444\n",
      "Epoch 14,  Batch 240:  Loss:     1.2876 Validation Accuracy: 0.527778\n",
      "Epoch15 start:\n",
      "Epoch 15,  Batch 0:  Loss:     1.2940 Validation Accuracy: 0.583333\n",
      "Epoch 15,  Batch 20:  Loss:     1.1375 Validation Accuracy: 0.666667\n",
      "Epoch 15,  Batch 40:  Loss:     1.3622 Validation Accuracy: 0.555556\n",
      "Epoch 15,  Batch 60:  Loss:     1.2217 Validation Accuracy: 0.611111\n",
      "Epoch 15,  Batch 80:  Loss:     1.4102 Validation Accuracy: 0.500000\n",
      "Epoch 15,  Batch 100:  Loss:     1.3450 Validation Accuracy: 0.583333\n",
      "Epoch 15,  Batch 120:  Loss:     1.2940 Validation Accuracy: 0.555556\n",
      "Epoch 15,  Batch 140:  Loss:     1.0626 Validation Accuracy: 0.638889\n",
      "Epoch 15,  Batch 160:  Loss:     1.2867 Validation Accuracy: 0.500000\n",
      "Epoch 15,  Batch 180:  Loss:     1.0500 Validation Accuracy: 0.666667\n",
      "Epoch 15,  Batch 200:  Loss:     1.1802 Validation Accuracy: 0.638889\n",
      "Epoch 15,  Batch 220:  Loss:     1.5313 Validation Accuracy: 0.500000\n",
      "Epoch 15,  Batch 240:  Loss:     1.4433 Validation Accuracy: 0.500000\n",
      "Epoch16 start:\n",
      "Epoch 16,  Batch 0:  Loss:     1.1148 Validation Accuracy: 0.611111\n",
      "Epoch 16,  Batch 20:  Loss:     1.2204 Validation Accuracy: 0.611111\n",
      "Epoch 16,  Batch 40:  Loss:     1.2761 Validation Accuracy: 0.611111\n",
      "Epoch 16,  Batch 60:  Loss:     1.5277 Validation Accuracy: 0.583333\n",
      "Epoch 16,  Batch 80:  Loss:     1.2033 Validation Accuracy: 0.527778\n",
      "Epoch 16,  Batch 100:  Loss:     1.3526 Validation Accuracy: 0.500000\n",
      "Epoch 16,  Batch 120:  Loss:     1.1496 Validation Accuracy: 0.638889\n",
      "Epoch 16,  Batch 140:  Loss:     1.0225 Validation Accuracy: 0.638889\n",
      "Epoch 16,  Batch 160:  Loss:     1.2854 Validation Accuracy: 0.500000\n",
      "Epoch 16,  Batch 180:  Loss:     1.2849 Validation Accuracy: 0.555556\n",
      "Epoch 16,  Batch 200:  Loss:     1.3094 Validation Accuracy: 0.583333\n",
      "Epoch 16,  Batch 220:  Loss:     0.8821 Validation Accuracy: 0.638889\n",
      "Epoch 16,  Batch 240:  Loss:     1.1460 Validation Accuracy: 0.611111\n",
      "Epoch17 start:\n",
      "Epoch 17,  Batch 0:  Loss:     1.1320 Validation Accuracy: 0.638889\n",
      "Epoch 17,  Batch 20:  Loss:     0.9150 Validation Accuracy: 0.638889\n",
      "Epoch 17,  Batch 40:  Loss:     1.2627 Validation Accuracy: 0.583333\n",
      "Epoch 17,  Batch 60:  Loss:     1.1823 Validation Accuracy: 0.527778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17,  Batch 80:  Loss:     1.3268 Validation Accuracy: 0.472222\n",
      "Epoch 17,  Batch 100:  Loss:     1.2928 Validation Accuracy: 0.638889\n",
      "Epoch 17,  Batch 120:  Loss:     1.1937 Validation Accuracy: 0.500000\n",
      "Epoch 17,  Batch 140:  Loss:     1.1237 Validation Accuracy: 0.611111\n",
      "Epoch 17,  Batch 160:  Loss:     1.0009 Validation Accuracy: 0.694444\n",
      "Epoch 17,  Batch 180:  Loss:     1.1928 Validation Accuracy: 0.666667\n",
      "Epoch 17,  Batch 200:  Loss:     1.2393 Validation Accuracy: 0.611111\n",
      "Epoch 17,  Batch 220:  Loss:     1.2580 Validation Accuracy: 0.555556\n",
      "Epoch 17,  Batch 240:  Loss:     1.0508 Validation Accuracy: 0.694444\n",
      "Epoch18 start:\n",
      "Epoch 18,  Batch 0:  Loss:     1.1503 Validation Accuracy: 0.638889\n",
      "Epoch 18,  Batch 20:  Loss:     1.2576 Validation Accuracy: 0.638889\n",
      "Epoch 18,  Batch 40:  Loss:     1.1964 Validation Accuracy: 0.500000\n",
      "Epoch 18,  Batch 60:  Loss:     1.3293 Validation Accuracy: 0.388889\n",
      "Epoch 18,  Batch 80:  Loss:     1.3734 Validation Accuracy: 0.555556\n",
      "Epoch 18,  Batch 100:  Loss:     1.1348 Validation Accuracy: 0.555556\n",
      "Epoch 18,  Batch 120:  Loss:     0.9766 Validation Accuracy: 0.666667\n",
      "Epoch 18,  Batch 140:  Loss:     1.2841 Validation Accuracy: 0.611111\n",
      "Epoch 18,  Batch 160:  Loss:     0.9134 Validation Accuracy: 0.750000\n",
      "Epoch 18,  Batch 180:  Loss:     1.0446 Validation Accuracy: 0.750000\n",
      "Epoch 18,  Batch 200:  Loss:     1.1804 Validation Accuracy: 0.527778\n",
      "Epoch 18,  Batch 220:  Loss:     1.2854 Validation Accuracy: 0.583333\n",
      "Epoch 18,  Batch 240:  Loss:     1.2253 Validation Accuracy: 0.500000\n",
      "Epoch19 start:\n",
      "Epoch 19,  Batch 0:  Loss:     1.2144 Validation Accuracy: 0.638889\n",
      "Epoch 19,  Batch 20:  Loss:     0.9405 Validation Accuracy: 0.750000\n",
      "Epoch 19,  Batch 40:  Loss:     1.2041 Validation Accuracy: 0.694444\n",
      "Epoch 19,  Batch 60:  Loss:     1.0776 Validation Accuracy: 0.583333\n",
      "Epoch 19,  Batch 80:  Loss:     1.3762 Validation Accuracy: 0.500000\n",
      "Epoch 19,  Batch 100:  Loss:     1.1757 Validation Accuracy: 0.583333\n",
      "Epoch 19,  Batch 120:  Loss:     1.3424 Validation Accuracy: 0.555556\n",
      "Epoch 19,  Batch 140:  Loss:     1.2419 Validation Accuracy: 0.527778\n",
      "Epoch 19,  Batch 160:  Loss:     1.2268 Validation Accuracy: 0.527778\n",
      "Epoch 19,  Batch 180:  Loss:     1.5317 Validation Accuracy: 0.555556\n",
      "Epoch 19,  Batch 200:  Loss:     1.3860 Validation Accuracy: 0.555556\n",
      "Epoch 19,  Batch 220:  Loss:     0.9297 Validation Accuracy: 0.638889\n",
      "Epoch 19,  Batch 240:  Loss:     1.1991 Validation Accuracy: 0.694444\n",
      "INFO:tensorflow:Restoring parameters from ./full_net_word_vec200\n",
      "batch output len:8960 output:[9, 16, 17, 12, 1, 13, 14, 15, 8, 10, 2, 5, 0, 18, 10, 3, 9, 13, 5, 10, 15, 0, 5, 18, 2, 4, 10, 10, 0, 15, 3, 5, 0, 10, 0, 0, 17, 11, 14, 10, 16, 13, 15, 0, 5, 7, 11, 14, 11, 12, 13, 2, 2, 3, 11, 16, 0, 2, 1, 11, 15, 5, 5, 18, 10, 16, 2, 5, 8, 14, 12, 11, 13, 9, 14, 2, 18, 10, 10, 8, 13, 2, 9, 12, 10, 0, 16, 8, 5, 15, 1, 16, 18, 1, 8, 4, 12, 10, 17, 1]\n",
      "x_train Testing Accuracy: 0.6111607142857143\n",
      "\n",
      "x_train Testing F1 Macro 0.5902161163819304    F1 Micro 0.6111607142857143\n",
      "x_val Testing Accuracy: 0.5101102941176471\n",
      "\n",
      "x_val Testing F1 Macro 0.48825296957941033    F1 Micro 0.5101102941176471\n",
      "Test _X Testing Accuracy: 0.46794181034482757\n",
      "\n",
      "test:[10, 1, 9, 11, 12, 16, 10, 2, 16, 11]\n",
      "Test _X Testing F1 Macro 0.44597684418524    F1 Micro 0.46794181034482757\n"
     ]
    }
   ],
   "source": [
    "model_path = './full_net_word_vec200'\n",
    "training_full_net(x_train_average_word_vec200, y_train_lb, [200], 20, model_path)\n",
    "\n",
    "test_model (model_path, x_train_average_word_vec200, y_train_lb, x_val_average_word_vec200, y_val_lb, \n",
    "           test_x_average_word_vec200, test_y_lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc2vec200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "<module 'tensorflow' from '/home/chronosvv/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/__init__.py'>\n",
      "Epoch0 start:\n",
      "Epoch  0,  Batch 0:  Loss:     2.6269 Validation Accuracy: 0.305556\n",
      "Epoch  0,  Batch 20:  Loss:     2.6958 Validation Accuracy: 0.277778\n",
      "Epoch  0,  Batch 40:  Loss:     2.5916 Validation Accuracy: 0.361111\n",
      "Epoch  0,  Batch 60:  Loss:     2.2536 Validation Accuracy: 0.500000\n",
      "Epoch  0,  Batch 80:  Loss:     2.2695 Validation Accuracy: 0.388889\n",
      "Epoch  0,  Batch 100:  Loss:     1.5219 Validation Accuracy: 0.444444\n",
      "Epoch  0,  Batch 120:  Loss:     1.8888 Validation Accuracy: 0.500000\n",
      "Epoch  0,  Batch 140:  Loss:     1.7329 Validation Accuracy: 0.444444\n",
      "Epoch  0,  Batch 160:  Loss:     1.5271 Validation Accuracy: 0.583333\n",
      "Epoch  0,  Batch 180:  Loss:     1.1286 Validation Accuracy: 0.638889\n",
      "Epoch  0,  Batch 200:  Loss:     1.5405 Validation Accuracy: 0.611111\n",
      "Epoch  0,  Batch 220:  Loss:     1.5424 Validation Accuracy: 0.638889\n",
      "Epoch  0,  Batch 240:  Loss:     1.4365 Validation Accuracy: 0.555556\n",
      "Epoch1 start:\n",
      "Epoch  1,  Batch 0:  Loss:     1.3956 Validation Accuracy: 0.611111\n",
      "Epoch  1,  Batch 20:  Loss:     1.2969 Validation Accuracy: 0.694444\n",
      "Epoch  1,  Batch 40:  Loss:     1.4652 Validation Accuracy: 0.527778\n",
      "Epoch  1,  Batch 60:  Loss:     1.3518 Validation Accuracy: 0.583333\n",
      "Epoch  1,  Batch 80:  Loss:     1.3190 Validation Accuracy: 0.611111\n",
      "Epoch  1,  Batch 100:  Loss:     1.2271 Validation Accuracy: 0.638889\n",
      "Epoch  1,  Batch 120:  Loss:     1.3405 Validation Accuracy: 0.611111\n",
      "Epoch  1,  Batch 140:  Loss:     1.1319 Validation Accuracy: 0.527778\n",
      "Epoch  1,  Batch 160:  Loss:     1.6154 Validation Accuracy: 0.500000\n",
      "Epoch  1,  Batch 180:  Loss:     1.6110 Validation Accuracy: 0.527778\n",
      "Epoch  1,  Batch 200:  Loss:     1.1279 Validation Accuracy: 0.583333\n",
      "Epoch  1,  Batch 220:  Loss:     1.0939 Validation Accuracy: 0.666667\n",
      "Epoch  1,  Batch 240:  Loss:     0.9660 Validation Accuracy: 0.694444\n",
      "Epoch2 start:\n",
      "Epoch  2,  Batch 0:  Loss:     1.2063 Validation Accuracy: 0.694444\n",
      "Epoch  2,  Batch 20:  Loss:     1.3112 Validation Accuracy: 0.583333\n",
      "Epoch  2,  Batch 40:  Loss:     0.7589 Validation Accuracy: 0.861111\n",
      "Epoch  2,  Batch 60:  Loss:     0.9251 Validation Accuracy: 0.777778\n",
      "Epoch  2,  Batch 80:  Loss:     1.2524 Validation Accuracy: 0.527778\n",
      "Epoch  2,  Batch 100:  Loss:     1.0370 Validation Accuracy: 0.666667\n",
      "Epoch  2,  Batch 120:  Loss:     0.8252 Validation Accuracy: 0.722222\n",
      "Epoch  2,  Batch 140:  Loss:     1.2677 Validation Accuracy: 0.555556\n",
      "Epoch  2,  Batch 160:  Loss:     1.1029 Validation Accuracy: 0.611111\n",
      "Epoch  2,  Batch 180:  Loss:     1.1403 Validation Accuracy: 0.638889\n",
      "Epoch  2,  Batch 200:  Loss:     1.2138 Validation Accuracy: 0.611111\n",
      "Epoch  2,  Batch 220:  Loss:     1.0636 Validation Accuracy: 0.666667\n",
      "Epoch  2,  Batch 240:  Loss:     1.2926 Validation Accuracy: 0.583333\n",
      "Epoch3 start:\n",
      "Epoch  3,  Batch 0:  Loss:     0.9662 Validation Accuracy: 0.666667\n",
      "Epoch  3,  Batch 20:  Loss:     0.9832 Validation Accuracy: 0.722222\n",
      "Epoch  3,  Batch 40:  Loss:     0.9809 Validation Accuracy: 0.638889\n",
      "Epoch  3,  Batch 60:  Loss:     1.0795 Validation Accuracy: 0.638889\n",
      "Epoch  3,  Batch 80:  Loss:     1.2265 Validation Accuracy: 0.666667\n",
      "Epoch  3,  Batch 100:  Loss:     1.0796 Validation Accuracy: 0.638889\n",
      "Epoch  3,  Batch 120:  Loss:     1.0143 Validation Accuracy: 0.666667\n",
      "Epoch  3,  Batch 140:  Loss:     0.7191 Validation Accuracy: 0.805556\n",
      "Epoch  3,  Batch 160:  Loss:     1.2408 Validation Accuracy: 0.666667\n",
      "Epoch  3,  Batch 180:  Loss:     0.7842 Validation Accuracy: 0.777778\n",
      "Epoch  3,  Batch 200:  Loss:     1.0256 Validation Accuracy: 0.638889\n",
      "Epoch  3,  Batch 220:  Loss:     0.8799 Validation Accuracy: 0.722222\n",
      "Epoch  3,  Batch 240:  Loss:     1.2097 Validation Accuracy: 0.666667\n",
      "Epoch4 start:\n",
      "Epoch  4,  Batch 0:  Loss:     0.6436 Validation Accuracy: 0.805556\n",
      "Epoch  4,  Batch 20:  Loss:     1.0445 Validation Accuracy: 0.694444\n",
      "Epoch  4,  Batch 40:  Loss:     0.8278 Validation Accuracy: 0.638889\n",
      "Epoch  4,  Batch 60:  Loss:     1.2310 Validation Accuracy: 0.583333\n",
      "Epoch  4,  Batch 80:  Loss:     0.9638 Validation Accuracy: 0.750000\n",
      "Epoch  4,  Batch 100:  Loss:     0.7763 Validation Accuracy: 0.861111\n",
      "Epoch  4,  Batch 120:  Loss:     0.8400 Validation Accuracy: 0.750000\n",
      "Epoch  4,  Batch 140:  Loss:     1.0493 Validation Accuracy: 0.722222\n",
      "Epoch  4,  Batch 160:  Loss:     0.5241 Validation Accuracy: 0.888889\n",
      "Epoch  4,  Batch 180:  Loss:     0.9224 Validation Accuracy: 0.750000\n",
      "Epoch  4,  Batch 200:  Loss:     0.8834 Validation Accuracy: 0.777778\n",
      "Epoch  4,  Batch 220:  Loss:     0.9412 Validation Accuracy: 0.722222\n",
      "Epoch  4,  Batch 240:  Loss:     1.0754 Validation Accuracy: 0.666667\n",
      "Epoch5 start:\n",
      "Epoch  5,  Batch 0:  Loss:     0.9176 Validation Accuracy: 0.666667\n",
      "Epoch  5,  Batch 20:  Loss:     0.8642 Validation Accuracy: 0.777778\n",
      "Epoch  5,  Batch 40:  Loss:     0.7196 Validation Accuracy: 0.750000\n",
      "Epoch  5,  Batch 60:  Loss:     0.8757 Validation Accuracy: 0.722222\n",
      "Epoch  5,  Batch 80:  Loss:     0.8412 Validation Accuracy: 0.777778\n",
      "Epoch  5,  Batch 100:  Loss:     0.7559 Validation Accuracy: 0.833333\n",
      "Epoch  5,  Batch 120:  Loss:     0.8362 Validation Accuracy: 0.805556\n",
      "Epoch  5,  Batch 140:  Loss:     0.8003 Validation Accuracy: 0.805556\n",
      "Epoch  5,  Batch 160:  Loss:     0.9297 Validation Accuracy: 0.722222\n",
      "Epoch  5,  Batch 180:  Loss:     1.0794 Validation Accuracy: 0.777778\n",
      "Epoch  5,  Batch 200:  Loss:     1.1165 Validation Accuracy: 0.666667\n",
      "Epoch  5,  Batch 220:  Loss:     0.9810 Validation Accuracy: 0.722222\n",
      "Epoch  5,  Batch 240:  Loss:     1.1271 Validation Accuracy: 0.638889\n",
      "Epoch6 start:\n",
      "Epoch  6,  Batch 0:  Loss:     0.8191 Validation Accuracy: 0.777778\n",
      "Epoch  6,  Batch 20:  Loss:     0.6523 Validation Accuracy: 0.861111\n",
      "Epoch  6,  Batch 40:  Loss:     0.8524 Validation Accuracy: 0.750000\n",
      "Epoch  6,  Batch 60:  Loss:     0.7031 Validation Accuracy: 0.833333\n",
      "Epoch  6,  Batch 80:  Loss:     0.7159 Validation Accuracy: 0.805556\n",
      "Epoch  6,  Batch 100:  Loss:     0.7037 Validation Accuracy: 0.833333\n",
      "Epoch  6,  Batch 120:  Loss:     0.7524 Validation Accuracy: 0.777778\n",
      "Epoch  6,  Batch 140:  Loss:     0.8739 Validation Accuracy: 0.722222\n",
      "Epoch  6,  Batch 160:  Loss:     0.4925 Validation Accuracy: 0.861111\n",
      "Epoch  6,  Batch 180:  Loss:     0.5188 Validation Accuracy: 0.833333\n",
      "Epoch  6,  Batch 200:  Loss:     0.4997 Validation Accuracy: 0.888889\n",
      "Epoch  6,  Batch 220:  Loss:     1.2179 Validation Accuracy: 0.611111\n",
      "Epoch  6,  Batch 240:  Loss:     0.6612 Validation Accuracy: 0.805556\n",
      "Epoch7 start:\n",
      "Epoch  7,  Batch 0:  Loss:     0.6582 Validation Accuracy: 0.833333\n",
      "Epoch  7,  Batch 20:  Loss:     0.5252 Validation Accuracy: 0.833333\n",
      "Epoch  7,  Batch 40:  Loss:     0.9509 Validation Accuracy: 0.666667\n",
      "Epoch  7,  Batch 60:  Loss:     0.8422 Validation Accuracy: 0.750000\n",
      "Epoch  7,  Batch 80:  Loss:     0.5630 Validation Accuracy: 0.861111\n",
      "Epoch  7,  Batch 100:  Loss:     0.7242 Validation Accuracy: 0.833333\n",
      "Epoch  7,  Batch 120:  Loss:     0.6448 Validation Accuracy: 0.805556\n",
      "Epoch  7,  Batch 140:  Loss:     0.9033 Validation Accuracy: 0.750000\n",
      "Epoch  7,  Batch 160:  Loss:     0.4275 Validation Accuracy: 0.861111\n",
      "Epoch  7,  Batch 180:  Loss:     0.3952 Validation Accuracy: 0.916667\n",
      "Epoch  7,  Batch 200:  Loss:     0.8416 Validation Accuracy: 0.722222\n",
      "Epoch  7,  Batch 220:  Loss:     0.7821 Validation Accuracy: 0.777778\n",
      "Epoch  7,  Batch 240:  Loss:     0.6069 Validation Accuracy: 0.861111\n",
      "Epoch8 start:\n",
      "Epoch  8,  Batch 0:  Loss:     0.5559 Validation Accuracy: 0.833333\n",
      "Epoch  8,  Batch 20:  Loss:     0.2661 Validation Accuracy: 0.972222\n",
      "Epoch  8,  Batch 40:  Loss:     0.8520 Validation Accuracy: 0.722222\n",
      "Epoch  8,  Batch 60:  Loss:     0.7815 Validation Accuracy: 0.722222\n",
      "Epoch  8,  Batch 80:  Loss:     0.4292 Validation Accuracy: 0.861111\n",
      "Epoch  8,  Batch 100:  Loss:     0.7703 Validation Accuracy: 0.805556\n",
      "Epoch  8,  Batch 120:  Loss:     0.5916 Validation Accuracy: 0.861111\n",
      "Epoch  8,  Batch 140:  Loss:     0.4905 Validation Accuracy: 0.861111\n",
      "Epoch  8,  Batch 160:  Loss:     0.5686 Validation Accuracy: 0.833333\n",
      "Epoch  8,  Batch 180:  Loss:     0.4750 Validation Accuracy: 0.861111\n",
      "Epoch  8,  Batch 200:  Loss:     0.4758 Validation Accuracy: 0.888889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8,  Batch 220:  Loss:     0.5156 Validation Accuracy: 0.888889\n",
      "Epoch  8,  Batch 240:  Loss:     0.6491 Validation Accuracy: 0.805556\n",
      "Epoch9 start:\n",
      "Epoch  9,  Batch 0:  Loss:     0.7950 Validation Accuracy: 0.777778\n",
      "Epoch  9,  Batch 20:  Loss:     0.6027 Validation Accuracy: 0.888889\n",
      "Epoch  9,  Batch 40:  Loss:     1.0028 Validation Accuracy: 0.694444\n",
      "Epoch  9,  Batch 60:  Loss:     0.7767 Validation Accuracy: 0.833333\n",
      "Epoch  9,  Batch 80:  Loss:     0.5748 Validation Accuracy: 0.805556\n",
      "Epoch  9,  Batch 100:  Loss:     0.5938 Validation Accuracy: 0.722222\n",
      "Epoch  9,  Batch 120:  Loss:     0.5410 Validation Accuracy: 0.888889\n",
      "Epoch  9,  Batch 140:  Loss:     0.8094 Validation Accuracy: 0.777778\n",
      "Epoch  9,  Batch 160:  Loss:     0.5881 Validation Accuracy: 0.888889\n",
      "Epoch  9,  Batch 180:  Loss:     0.3386 Validation Accuracy: 0.972222\n",
      "Epoch  9,  Batch 200:  Loss:     0.5469 Validation Accuracy: 0.861111\n",
      "Epoch  9,  Batch 220:  Loss:     0.9770 Validation Accuracy: 0.750000\n",
      "Epoch  9,  Batch 240:  Loss:     0.6171 Validation Accuracy: 0.777778\n",
      "Epoch10 start:\n",
      "Epoch 10,  Batch 0:  Loss:     0.4318 Validation Accuracy: 0.916667\n",
      "Epoch 10,  Batch 20:  Loss:     0.5009 Validation Accuracy: 0.888889\n",
      "Epoch 10,  Batch 40:  Loss:     0.7455 Validation Accuracy: 0.777778\n",
      "Epoch 10,  Batch 60:  Loss:     0.3768 Validation Accuracy: 0.833333\n",
      "Epoch 10,  Batch 80:  Loss:     0.5517 Validation Accuracy: 0.888889\n",
      "Epoch 10,  Batch 100:  Loss:     0.4994 Validation Accuracy: 0.888889\n",
      "Epoch 10,  Batch 120:  Loss:     0.7674 Validation Accuracy: 0.805556\n",
      "Epoch 10,  Batch 140:  Loss:     0.6262 Validation Accuracy: 0.861111\n",
      "Epoch 10,  Batch 160:  Loss:     0.9809 Validation Accuracy: 0.777778\n",
      "Epoch 10,  Batch 180:  Loss:     0.3209 Validation Accuracy: 0.916667\n",
      "Epoch 10,  Batch 200:  Loss:     0.4801 Validation Accuracy: 0.833333\n",
      "Epoch 10,  Batch 220:  Loss:     0.2648 Validation Accuracy: 0.944444\n",
      "Epoch 10,  Batch 240:  Loss:     0.3811 Validation Accuracy: 0.888889\n",
      "Epoch11 start:\n",
      "Epoch 11,  Batch 0:  Loss:     0.4218 Validation Accuracy: 0.916667\n",
      "Epoch 11,  Batch 20:  Loss:     0.2090 Validation Accuracy: 0.972222\n",
      "Epoch 11,  Batch 40:  Loss:     0.4189 Validation Accuracy: 0.916667\n",
      "Epoch 11,  Batch 60:  Loss:     0.6345 Validation Accuracy: 0.777778\n",
      "Epoch 11,  Batch 80:  Loss:     0.5635 Validation Accuracy: 0.888889\n",
      "Epoch 11,  Batch 100:  Loss:     0.8410 Validation Accuracy: 0.750000\n",
      "Epoch 11,  Batch 120:  Loss:     0.3276 Validation Accuracy: 0.944444\n",
      "Epoch 11,  Batch 140:  Loss:     0.3855 Validation Accuracy: 0.861111\n",
      "Epoch 11,  Batch 160:  Loss:     0.2891 Validation Accuracy: 0.888889\n",
      "Epoch 11,  Batch 180:  Loss:     0.4767 Validation Accuracy: 0.861111\n",
      "Epoch 11,  Batch 200:  Loss:     0.3428 Validation Accuracy: 0.916667\n",
      "Epoch 11,  Batch 220:  Loss:     0.3145 Validation Accuracy: 0.916667\n",
      "Epoch 11,  Batch 240:  Loss:     0.6900 Validation Accuracy: 0.833333\n",
      "Epoch12 start:\n",
      "Epoch 12,  Batch 0:  Loss:     0.2531 Validation Accuracy: 0.972222\n",
      "Epoch 12,  Batch 20:  Loss:     0.3031 Validation Accuracy: 0.916667\n",
      "Epoch 12,  Batch 40:  Loss:     0.6533 Validation Accuracy: 0.833333\n",
      "Epoch 12,  Batch 60:  Loss:     0.7353 Validation Accuracy: 0.805556\n",
      "Epoch 12,  Batch 80:  Loss:     0.6349 Validation Accuracy: 0.777778\n",
      "Epoch 12,  Batch 100:  Loss:     0.3635 Validation Accuracy: 0.916667\n",
      "Epoch 12,  Batch 120:  Loss:     0.4840 Validation Accuracy: 0.888889\n",
      "Epoch 12,  Batch 140:  Loss:     0.5411 Validation Accuracy: 0.888889\n",
      "Epoch 12,  Batch 160:  Loss:     0.4700 Validation Accuracy: 0.888889\n",
      "Epoch 12,  Batch 180:  Loss:     0.4646 Validation Accuracy: 0.888889\n",
      "Epoch 12,  Batch 200:  Loss:     0.3262 Validation Accuracy: 0.916667\n",
      "Epoch 12,  Batch 220:  Loss:     0.4717 Validation Accuracy: 0.888889\n",
      "Epoch 12,  Batch 240:  Loss:     0.2895 Validation Accuracy: 0.916667\n",
      "Epoch13 start:\n",
      "Epoch 13,  Batch 0:  Loss:     0.4020 Validation Accuracy: 0.916667\n",
      "Epoch 13,  Batch 20:  Loss:     0.3136 Validation Accuracy: 0.916667\n",
      "Epoch 13,  Batch 40:  Loss:     0.3154 Validation Accuracy: 0.944444\n",
      "Epoch 13,  Batch 60:  Loss:     0.3697 Validation Accuracy: 0.833333\n",
      "Epoch 13,  Batch 80:  Loss:     0.4645 Validation Accuracy: 0.944444\n",
      "Epoch 13,  Batch 100:  Loss:     0.1979 Validation Accuracy: 1.000000\n",
      "Epoch 13,  Batch 120:  Loss:     0.3858 Validation Accuracy: 0.888889\n",
      "Epoch 13,  Batch 140:  Loss:     0.1723 Validation Accuracy: 0.972222\n",
      "Epoch 13,  Batch 160:  Loss:     0.5830 Validation Accuracy: 0.833333\n",
      "Epoch 13,  Batch 180:  Loss:     0.0914 Validation Accuracy: 1.000000\n",
      "Epoch 13,  Batch 200:  Loss:     0.2336 Validation Accuracy: 0.972222\n",
      "Epoch 13,  Batch 220:  Loss:     0.5946 Validation Accuracy: 0.861111\n",
      "Epoch 13,  Batch 240:  Loss:     0.5015 Validation Accuracy: 0.888889\n",
      "Epoch14 start:\n",
      "Epoch 14,  Batch 0:  Loss:     0.2206 Validation Accuracy: 0.944444\n",
      "Epoch 14,  Batch 20:  Loss:     0.4652 Validation Accuracy: 0.861111\n",
      "Epoch 14,  Batch 40:  Loss:     0.3609 Validation Accuracy: 0.916667\n",
      "Epoch 14,  Batch 60:  Loss:     0.3960 Validation Accuracy: 0.861111\n",
      "Epoch 14,  Batch 80:  Loss:     0.4367 Validation Accuracy: 0.861111\n",
      "Epoch 14,  Batch 100:  Loss:     0.4614 Validation Accuracy: 0.944444\n",
      "Epoch 14,  Batch 120:  Loss:     0.2829 Validation Accuracy: 0.944444\n",
      "Epoch 14,  Batch 140:  Loss:     0.5503 Validation Accuracy: 0.833333\n",
      "Epoch 14,  Batch 160:  Loss:     0.5162 Validation Accuracy: 0.861111\n",
      "Epoch 14,  Batch 180:  Loss:     0.2129 Validation Accuracy: 0.944444\n",
      "Epoch 14,  Batch 200:  Loss:     0.4665 Validation Accuracy: 0.833333\n",
      "Epoch 14,  Batch 220:  Loss:     0.3172 Validation Accuracy: 0.888889\n",
      "Epoch 14,  Batch 240:  Loss:     0.4124 Validation Accuracy: 0.916667\n",
      "Epoch15 start:\n",
      "Epoch 15,  Batch 0:  Loss:     0.5535 Validation Accuracy: 0.833333\n",
      "Epoch 15,  Batch 20:  Loss:     0.2001 Validation Accuracy: 0.944444\n",
      "Epoch 15,  Batch 40:  Loss:     0.2272 Validation Accuracy: 0.944444\n",
      "Epoch 15,  Batch 60:  Loss:     0.3672 Validation Accuracy: 0.916667\n",
      "Epoch 15,  Batch 80:  Loss:     0.3185 Validation Accuracy: 0.916667\n",
      "Epoch 15,  Batch 100:  Loss:     0.1834 Validation Accuracy: 0.944444\n",
      "Epoch 15,  Batch 120:  Loss:     0.2979 Validation Accuracy: 0.916667\n",
      "Epoch 15,  Batch 140:  Loss:     0.4401 Validation Accuracy: 0.888889\n",
      "Epoch 15,  Batch 160:  Loss:     0.4005 Validation Accuracy: 0.861111\n",
      "Epoch 15,  Batch 180:  Loss:     0.4267 Validation Accuracy: 0.861111\n",
      "Epoch 15,  Batch 200:  Loss:     0.2463 Validation Accuracy: 0.916667\n",
      "Epoch 15,  Batch 220:  Loss:     0.2211 Validation Accuracy: 0.972222\n",
      "Epoch 15,  Batch 240:  Loss:     0.4784 Validation Accuracy: 0.888889\n",
      "Epoch16 start:\n",
      "Epoch 16,  Batch 0:  Loss:     0.1083 Validation Accuracy: 0.972222\n",
      "Epoch 16,  Batch 20:  Loss:     0.1621 Validation Accuracy: 0.972222\n",
      "Epoch 16,  Batch 40:  Loss:     0.3342 Validation Accuracy: 0.916667\n",
      "Epoch 16,  Batch 60:  Loss:     0.2045 Validation Accuracy: 0.972222\n",
      "Epoch 16,  Batch 80:  Loss:     0.3035 Validation Accuracy: 1.000000\n",
      "Epoch 16,  Batch 100:  Loss:     0.1765 Validation Accuracy: 1.000000\n",
      "Epoch 16,  Batch 120:  Loss:     0.4171 Validation Accuracy: 0.833333\n",
      "Epoch 16,  Batch 140:  Loss:     0.2837 Validation Accuracy: 0.916667\n",
      "Epoch 16,  Batch 160:  Loss:     0.1843 Validation Accuracy: 0.972222\n",
      "Epoch 16,  Batch 180:  Loss:     0.2847 Validation Accuracy: 0.944444\n",
      "Epoch 16,  Batch 200:  Loss:     0.2866 Validation Accuracy: 0.916667\n",
      "Epoch 16,  Batch 220:  Loss:     0.5937 Validation Accuracy: 0.833333\n",
      "Epoch 16,  Batch 240:  Loss:     0.3591 Validation Accuracy: 0.888889\n",
      "Epoch17 start:\n",
      "Epoch 17,  Batch 0:  Loss:     0.1749 Validation Accuracy: 0.944444\n",
      "Epoch 17,  Batch 20:  Loss:     0.0335 Validation Accuracy: 1.000000\n",
      "Epoch 17,  Batch 40:  Loss:     0.0715 Validation Accuracy: 1.000000\n",
      "Epoch 17,  Batch 60:  Loss:     0.5551 Validation Accuracy: 0.861111\n",
      "Epoch 17,  Batch 80:  Loss:     0.2290 Validation Accuracy: 0.972222\n",
      "Epoch 17,  Batch 100:  Loss:     0.3237 Validation Accuracy: 0.888889\n",
      "Epoch 17,  Batch 120:  Loss:     0.2149 Validation Accuracy: 0.972222\n",
      "Epoch 17,  Batch 140:  Loss:     0.2701 Validation Accuracy: 0.944444\n",
      "Epoch 17,  Batch 160:  Loss:     0.1560 Validation Accuracy: 0.972222\n",
      "Epoch 17,  Batch 180:  Loss:     0.1805 Validation Accuracy: 0.944444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17,  Batch 200:  Loss:     0.3623 Validation Accuracy: 0.861111\n",
      "Epoch 17,  Batch 220:  Loss:     0.1625 Validation Accuracy: 0.944444\n",
      "Epoch 17,  Batch 240:  Loss:     0.3735 Validation Accuracy: 0.861111\n",
      "Epoch18 start:\n",
      "Epoch 18,  Batch 0:  Loss:     0.4994 Validation Accuracy: 0.833333\n",
      "Epoch 18,  Batch 20:  Loss:     0.3934 Validation Accuracy: 0.916667\n",
      "Epoch 18,  Batch 40:  Loss:     0.1643 Validation Accuracy: 0.972222\n",
      "Epoch 18,  Batch 60:  Loss:     0.0684 Validation Accuracy: 1.000000\n",
      "Epoch 18,  Batch 80:  Loss:     0.2605 Validation Accuracy: 0.972222\n",
      "Epoch 18,  Batch 100:  Loss:     0.1111 Validation Accuracy: 1.000000\n",
      "Epoch 18,  Batch 120:  Loss:     0.3042 Validation Accuracy: 0.916667\n",
      "Epoch 18,  Batch 140:  Loss:     0.2550 Validation Accuracy: 0.944444\n",
      "Epoch 18,  Batch 160:  Loss:     0.4593 Validation Accuracy: 0.861111\n",
      "Epoch 18,  Batch 180:  Loss:     0.0421 Validation Accuracy: 1.000000\n",
      "Epoch 18,  Batch 200:  Loss:     0.2389 Validation Accuracy: 0.916667\n",
      "Epoch 18,  Batch 220:  Loss:     0.3475 Validation Accuracy: 0.888889\n",
      "Epoch 18,  Batch 240:  Loss:     0.2178 Validation Accuracy: 0.916667\n",
      "Epoch19 start:\n",
      "Epoch 19,  Batch 0:  Loss:     0.1563 Validation Accuracy: 0.972222\n",
      "Epoch 19,  Batch 20:  Loss:     0.1182 Validation Accuracy: 0.972222\n",
      "Epoch 19,  Batch 40:  Loss:     0.4516 Validation Accuracy: 0.888889\n",
      "Epoch 19,  Batch 60:  Loss:     0.2608 Validation Accuracy: 0.916667\n",
      "Epoch 19,  Batch 80:  Loss:     0.2643 Validation Accuracy: 0.916667\n",
      "Epoch 19,  Batch 100:  Loss:     0.1287 Validation Accuracy: 1.000000\n",
      "Epoch 19,  Batch 120:  Loss:     0.2082 Validation Accuracy: 0.944444\n",
      "Epoch 19,  Batch 140:  Loss:     0.1362 Validation Accuracy: 0.972222\n",
      "Epoch 19,  Batch 160:  Loss:     0.2997 Validation Accuracy: 0.916667\n",
      "Epoch 19,  Batch 180:  Loss:     0.1481 Validation Accuracy: 0.944444\n",
      "Epoch 19,  Batch 200:  Loss:     0.2114 Validation Accuracy: 0.944444\n",
      "Epoch 19,  Batch 220:  Loss:     0.2133 Validation Accuracy: 0.944444\n",
      "Epoch 19,  Batch 240:  Loss:     0.1581 Validation Accuracy: 1.000000\n",
      "INFO:tensorflow:Restoring parameters from ./full_net_doc_vec200\n",
      "batch output len:8960 output:[8, 9, 18, 14, 12, 14, 0, 13, 9, 4, 4, 19, 10, 1, 8, 9, 6, 9, 14, 9, 15, 2, 2, 5, 5, 16, 16, 19, 13, 10, 3, 0, 6, 10, 9, 0, 10, 14, 10, 5, 2, 1, 4, 5, 9, 9, 4, 2, 15, 2, 4, 1, 6, 15, 7, 11, 9, 0, 12, 13, 4, 16, 10, 5, 12, 12, 15, 3, 0, 17, 16, 14, 19, 1, 9, 3, 3, 19, 3, 5, 0, 19, 17, 8, 13, 1, 17, 11, 12, 4, 11, 12, 10, 4, 13, 9, 16, 0, 19, 5]\n",
      "x_train Testing Accuracy: 0.9513392857142857\n",
      "\n",
      "x_train Testing F1 Macro 0.9516148461600128    F1 Micro 0.9513392857142857\n",
      "x_val Testing Accuracy: 0.6360294117647058\n",
      "\n",
      "x_val Testing F1 Macro 0.633316870799699    F1 Micro 0.6360294117647058\n",
      "Test _X Testing Accuracy: 0.5934806034482759\n",
      "\n",
      "test:[13, 8, 2, 12, 7, 14, 11, 12, 7, 5]\n",
      "Test _X Testing F1 Macro 0.5847590274605254    F1 Micro 0.5934806034482759\n"
     ]
    }
   ],
   "source": [
    "model_path = './full_net_doc_vec200'\n",
    "training_full_net(x_train_doc2vec200, y_train_lb, [200], 20, model_path)\n",
    "\n",
    "test_model (model_path, x_train_doc2vec200, y_train_lb, x_val_doc2vec200, y_val_lb, \n",
    "           test_x_doc2vec200, test_y_lb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
