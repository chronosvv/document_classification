{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, isdir, join\n",
    "import tarfile\n",
    "import gzip\n",
    "\n",
    "news20 = \"20news-bydate.tar.gz\"\n",
    "news20_path = 'news20'\n",
    "\n",
    "models_path = 'models'\n",
    "\n",
    "temp_path = 'temp'\n",
    "\n",
    "if not isdir(news20_path):\n",
    "    gz = gzip.GzipFile(news20)\n",
    "    tarfilename = news20.replace('.gz', '')\n",
    "    open(tarfilename, 'wb').write(gz.read())\n",
    "    \n",
    "    with tarfile.TarFile(tarfilename) as tarf:\n",
    "        tarf.extractall(news20_path)\n",
    "        tarf.close()\n",
    "    \n",
    "    gz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert isdir(news20_path), \"The news20 is not availaible\"\n",
    "\n",
    "train_path = news20_path + '/20news-bydate-train'\n",
    "test_path = news20_path + '/20news-bydate-test'\n",
    "\n",
    "def get_x_y():\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    \n",
    "    for foldername in os.listdir(train_path):\n",
    "        folder_path = join(train_path, foldername)\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            f = open(folder_path + '/' + filename, 'rb').read()\n",
    "            train_x.append(f)\n",
    "            train_y.append(foldername)\n",
    "            \n",
    "    for foldername in os.listdir(test_path):   \n",
    "        folder_path = join(test_path, foldername)\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            f = open(folder_path + '/' + filename, 'rb').read()\n",
    "            test_x.append(f)\n",
    "            test_y.append(foldername)\n",
    "            \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b\"From: mattf@cac.washington.edu (Matthew Freedman)\\nSubject: Non-Roman Font Availability\\nArticle-I.D.: shelley.1rmgleINNa0g\\nDistribution: world\\nOrganization: U.W. Information Systems\\nLines: 16\\nNNTP-Posting-Host: elvis.cac.washington.edu\\n\\nCan anybody tell me anything about the availibility of non-Roman fonts\\nfor X-Windows? Especially Unicode and/or han idiographic fonts.\\n\\nAlso, how about conversion tools for getting PC/Macintosh fonts into a\\nformat suitable for X? I would assume it is not too difficult for\\nbitmap fonts.\\n\\nThe FAQ's for this group and comp.fonts are not very helpful on these\\nquestions. \\n\\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\\n= Matthew M. Freedman                                                 =\\n= U. of Washington Information Systems       mattf@cac.washington.edu =\\n= 4545 15th Ave. NE; 3rd Floor               (206) 543-5593           =\\n= Seattle, WA  98105                                                  =\\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\\n\"]\n",
      "训练集一共有文档11314\n",
      "测试集一共有文档7532\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = get_x_y()\n",
    "print(test_x[:1])\n",
    "# print(train_y[1:11314:500])\n",
    "print(\"训练集一共有文档{}\".format(len(train_x)))\n",
    "print(\"测试集一共有文档{}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]]\n",
      "[5 5 5]\n",
      "[5 5 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(train_y)\n",
    "\n",
    "train_y_lb = lb.transform(train_y)\n",
    "test_y_lb = lb.transform(test_y)\n",
    "\n",
    "le = preprocessing.LabelEncoder() \n",
    "le.fit(train_y)\n",
    "train_y_le = le.transform(train_y)\n",
    "test_y_le = le.transform(test_y)\n",
    "\n",
    "print(test_y_lb[-2:])\n",
    "print(test_y_le[:3])\n",
    "print(train_y_le[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network parameters\n",
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 1000 # max number of words in a comment to use\n",
    "num_filters = 100 # the number of CNN filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "#把文档从字符串转为字符标量列表\n",
    "def get_train_sequence():\n",
    "    max_nb_token = 20000\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]_^`{|}~\\t\\n'\n",
    "    train_x, _, test_x, _ = get_x_y()    \n",
    "    tokenizer = Tokenizer(num_words = max_nb_token, filters = filters, split = \" \") #最大20，000个单词\n",
    "    tokenizer.fit_on_texts([x.decode('gbk', 'ignore') for x in train_x])\n",
    "    train_x_sequence = tokenizer.texts_to_sequences([x.decode('gbk', 'ignore') for x in train_x])\n",
    "    test_x_sequence = tokenizer.texts_to_sequences([x.decode('gbk', 'ignore') for x in test_x])\n",
    "    return train_x_sequence, test_x_sequence, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#=================Keras==============\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Conv1D, Conv2D,Embedding, Dropout, Activation, Permute\n",
    "from keras.layers import Bidirectional, MaxPooling1D, MaxPooling2D, Reshape, Flatten, Concatenate, BatchNormalization, GlobalMaxPool1D, GlobalMaxPool2D,SpatialDropout1D\n",
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, backend\n",
    "#=================gensim=============\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, tokenizer = get_train_sequence()\n",
    "\n",
    "glove_dir = \"glove_vector/\"\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(os.path.join(glove_dir, 'glove.6B.50d.txt')))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics  import f1_score\n",
    "#输入模型预测后再比较\n",
    "def get_f1_score(clf, input_data, target_data):\n",
    "    predict_data = clf.predict(input_data)\n",
    "    f1_macro = f1_score(target_data, predict_data,  average = 'macro')\n",
    "    f1_micro = f1_score(target_data, predict_data,  average = 'micro')\n",
    "    return f1_macro, f1_micro\n",
    "\n",
    "#直接比较输入输出\n",
    "def get_f1_score_pure(input_data, target_data):\n",
    "    f1_macro = f1_score(target_data, input_data,  average = 'macro')\n",
    "    f1_micro = f1_score(target_data, input_data,  average = 'micro')\n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
    "filter_sizes = [3, 4, 5]\n",
    "\n",
    "def get_model():    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Reshape((maxlen, embed_size, 1))(x)\n",
    "    \n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    \n",
    "    maxpool_0 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1))(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1))(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(maxlen - filter_sizes[2] + 1, 1))(conv_2)\n",
    "        \n",
    "    z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "        \n",
    "    outp = Dense(20, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器我这里用了adam\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping, TensorBoard,ReduceLROnPlateau\n",
    "\n",
    "#动态调节学习率\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "rlronp = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.004\n",
    "    drop = 0.6\n",
    "    epochs_drop = 5.0\n",
    "    lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "def train_cnn_network(model, x_train, y_train):\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "    model.fit(x_train, y_train, batch_size=32, validation_split=0.1, verbose = 2, shuffle = True, epochs=10, callbacks = [lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/10\n",
      " - 79s - loss: 0.1550 - acc: 0.9544 - val_loss: 0.0860 - val_acc: 0.9735\n",
      "Epoch 2/10\n",
      " - 77s - loss: 0.0839 - acc: 0.9718 - val_loss: 0.0627 - val_acc: 0.9803\n",
      "Epoch 3/10\n",
      " - 77s - loss: 0.0622 - acc: 0.9791 - val_loss: 0.0490 - val_acc: 0.9846\n",
      "Epoch 4/10\n",
      " - 77s - loss: 0.0480 - acc: 0.9842 - val_loss: 0.0502 - val_acc: 0.9844\n",
      "Epoch 5/10\n",
      " - 76s - loss: 0.0350 - acc: 0.9882 - val_loss: 0.0416 - val_acc: 0.9872\n",
      "Epoch 6/10\n",
      " - 77s - loss: 0.0300 - acc: 0.9899 - val_loss: 0.0397 - val_acc: 0.9874\n",
      "Epoch 7/10\n",
      " - 76s - loss: 0.0273 - acc: 0.9906 - val_loss: 0.0383 - val_acc: 0.9884\n",
      "Epoch 8/10\n",
      " - 77s - loss: 0.0243 - acc: 0.9917 - val_loss: 0.0397 - val_acc: 0.9883\n",
      "Epoch 9/10\n",
      " - 77s - loss: 0.0216 - acc: 0.9926 - val_loss: 0.0390 - val_acc: 0.9883\n",
      "Epoch 10/10\n",
      " - 76s - loss: 0.0170 - acc: 0.9942 - val_loss: 0.0381 - val_acc: 0.9885\n"
     ]
    }
   ],
   "source": [
    "#取文档序列作为输入\n",
    "from keras.preprocessing import sequence\n",
    "y_train = train_y_lb\n",
    "y_test = test_y_lb\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(\n",
    "    x_train, maxlen=maxlen, padding = \"post\", value = 0)\n",
    "x_test = sequence.pad_sequences(\n",
    "    x_test, maxlen=maxlen, padding = \"post\", value  = 0)\n",
    "\n",
    "model = get_model()\n",
    "train_cnn_network(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def eva_cnn_network(model, x_train, x_test):\n",
    "    begin_time = datetime.datetime.now()\n",
    "\n",
    "    train_predict_y = model.predict(x_train)\n",
    "    train_predict_y_nb = [np.argmax(doc) for doc in train_predict_y]\n",
    "    train_target_y_nb = [np.argmax(doc) for doc in y_train]\n",
    "    f1_macro_train_train, f1_micro_train_train = get_f1_score_pure(\n",
    "        train_predict_y_nb, train_target_y_nb)\n",
    "\n",
    "    test_predict_y = model.predict(x_test)\n",
    "    test_predict_y_nb = [np.argmax(doc) for doc in test_predict_y]\n",
    "    test_target_y_nb = [np.argmax(doc) for doc in y_test]\n",
    "    f1_macro_test, f1_micro_test = get_f1_score_pure(\n",
    "        test_predict_y_nb, test_target_y_nb)\n",
    "\n",
    "    print(\"F1 Macro on train data: {}, F1 Micro: {}; \\\n",
    "          test data:Macro{} Micro{}\".format(f1_macro_train_train, \n",
    "                                            f1_micro_train_train, \n",
    "                                            f1_macro_test, f1_micro_test))\n",
    "    print(\"测试CNN 网络一共花了{}时间\".format(\n",
    "        datetime.datetime.now() - begin_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro on train data: 0.9874831712885681, F1 Micro: 0.987537564079901;           test data:Macro0.8315883530964655 Micro0.837360594795539\n",
      "测试CNN 网络一共花了0:00:40.927720时间\n"
     ]
    }
   ],
   "source": [
    "eva_cnn_network(model, x_train, x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
